{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gml7jn98Chwt"
   },
   "source": [
    "## 🧠 SimpleCNN\n",
    "- Conv2D (3→32)\n",
    "- MaxPool2D\t(16x16x32)\n",
    "- Conv2D (32→64)\t(16x16x64)\n",
    "- MaxPool2D\t(8x8x64)\n",
    "- Flatten\t(4096)\n",
    "- FC(4096→128)\n",
    "- FC(128→10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51274,
     "status": "ok",
     "timestamp": 1744039397701,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "1tdO58AiCfnY",
    "outputId": "6d3f0d89-290f-4a43-fa0d-b0906b54cf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-325b337f7fb4>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "100%|██████████| 170M/170M [00:03<00:00, 53.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "SimpleCNN                                [128, 10]                 --\n",
      "├─Sequential: 1-1                        [128, 64, 8, 8]           --\n",
      "│    └─Conv2d: 2-1                       [128, 32, 32, 32]         896\n",
      "│    └─ReLU: 2-2                         [128, 32, 32, 32]         --\n",
      "│    └─MaxPool2d: 2-3                    [128, 32, 16, 16]         --\n",
      "│    └─Conv2d: 2-4                       [128, 64, 16, 16]         18,496\n",
      "│    └─ReLU: 2-5                         [128, 64, 16, 16]         --\n",
      "│    └─MaxPool2d: 2-6                    [128, 64, 8, 8]           --\n",
      "├─Sequential: 1-2                        [128, 10]                 --\n",
      "│    └─Flatten: 2-7                      [128, 4096]               --\n",
      "│    └─Linear: 2-8                       [128, 128]                524,416\n",
      "│    └─ReLU: 2-9                         [128, 128]                --\n",
      "│    └─Dropout: 2-10                     [128, 128]                --\n",
      "│    └─Linear: 2-11                      [128, 10]                 1,290\n",
      "==========================================================================================\n",
      "Total params: 545,098\n",
      "Trainable params: 545,098\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 790.81\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 50.47\n",
      "Params size (MB): 2.18\n",
      "Estimated Total Size (MB): 54.23\n",
      "==========================================================================================\n",
      "Validation - Precision: 0.5680, Recall: 0.5689, F1 Score: 0.5577\n",
      "Epoch [1/2], Train Loss: 1.5091, Val Loss: 1.2296, Val Acc: 56.91%\n",
      "New Best Model Saved! Validation Accuracy: 56.91%\n",
      "Validation - Precision: 0.6473, Recall: 0.6352, F1 Score: 0.6293\n",
      "Epoch [2/2], Train Loss: 1.0666, Val Loss: 1.0193, Val Acc: 63.48%\n",
      "New Best Model Saved! Validation Accuracy: 63.48%\n",
      "Test Accuracy: 63.36%\n",
      "Test Precision: 0.6489, Recall: 0.6336, F1 Score: 0.6289\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchinfo import summary\n",
    "\n",
    "# GPU 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 학습 데이터 증강 설정 (후반 학습용)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (CNN)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [B, 3, 32, 32] → [B, 32, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # → [B, 32, 16, 16]\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # → [B, 64, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # → [B, 64, 8, 8]\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),                                # → [B, 64*8*8]\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleCNN().to(DEVICE)\n",
    "\n",
    "# summary 호출\n",
    "print(summary(model, input_size=(BATCH_SIZE, 3, 32, 32)))\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # 반환:(최댓값, 인덱스)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    # load_model()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKGdKec92640"
   },
   "source": [
    "## 🧠 ResNet18\n",
    "- ResidualBlock\n",
    "\n",
    "```\n",
    "        입력 x\n",
    "          │\n",
    "          ├───────┐\n",
    "          │              │\n",
    "          ▼              ▼\n",
    "      Conv2D(3x3)     (shortcut)\n",
    "          ↓              │\n",
    "      BatchNorm           │\n",
    "          ↓              │\n",
    "         ReLU             │\n",
    "          ↓              │\n",
    "      Conv2D(3x3)         │\n",
    "          ↓              │\n",
    "       BatchNorm          │\n",
    "          ↓              │\n",
    "      처리된 결과 out ◄─-┘ ← identity (x 또는 변형된 x)\n",
    "             │\n",
    "             ▼\n",
    "         out + identity\n",
    "             │\n",
    "             ▼\n",
    "            ReLU\n",
    "             │\n",
    "             ▼\n",
    "         최종 출력\n",
    "```\n",
    "\n",
    "- CustomResNet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41271,
     "status": "ok",
     "timestamp": 1744039985390,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "Go7Ej6Ym27Le",
    "outputId": "9c549865-b964-4264-8dbb-766a2c475741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CustomResNet18                           [128, 10]                 --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,728\n",
      "├─BatchNorm2d: 1-2                       [128, 64, 32, 32]         128\n",
      "├─ReLU: 1-3                              [128, 64, 32, 32]         --\n",
      "├─Sequential: 1-4                        [128, 64, 32, 32]         --\n",
      "│    └─ResidualBlock: 2-1                [128, 64, 32, 32]         --\n",
      "│    │    └─Identity: 3-1                [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-2                  [128, 64, 32, 32]         36,864\n",
      "│    │    └─BatchNorm2d: 3-3             [128, 64, 32, 32]         128\n",
      "│    │    └─ReLU: 3-4                    [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-5                  [128, 64, 32, 32]         36,864\n",
      "│    │    └─BatchNorm2d: 3-6             [128, 64, 32, 32]         128\n",
      "│    │    └─ReLU: 3-7                    [128, 64, 32, 32]         --\n",
      "│    └─ResidualBlock: 2-2                [128, 64, 32, 32]         --\n",
      "│    │    └─Identity: 3-8                [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-9                  [128, 64, 32, 32]         36,864\n",
      "│    │    └─BatchNorm2d: 3-10            [128, 64, 32, 32]         128\n",
      "│    │    └─ReLU: 3-11                   [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-12                 [128, 64, 32, 32]         36,864\n",
      "│    │    └─BatchNorm2d: 3-13            [128, 64, 32, 32]         128\n",
      "│    │    └─ReLU: 3-14                   [128, 64, 32, 32]         --\n",
      "├─Sequential: 1-5                        [128, 128, 16, 16]        --\n",
      "│    └─ResidualBlock: 2-3                [128, 128, 16, 16]        --\n",
      "│    │    └─Sequential: 3-15             [128, 128, 16, 16]        8,448\n",
      "│    │    └─Conv2d: 3-16                 [128, 128, 16, 16]        73,728\n",
      "│    │    └─BatchNorm2d: 3-17            [128, 128, 16, 16]        256\n",
      "│    │    └─ReLU: 3-18                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-19                 [128, 128, 16, 16]        147,456\n",
      "│    │    └─BatchNorm2d: 3-20            [128, 128, 16, 16]        256\n",
      "│    │    └─ReLU: 3-21                   [128, 128, 16, 16]        --\n",
      "│    └─ResidualBlock: 2-4                [128, 128, 16, 16]        --\n",
      "│    │    └─Identity: 3-22               [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-23                 [128, 128, 16, 16]        147,456\n",
      "│    │    └─BatchNorm2d: 3-24            [128, 128, 16, 16]        256\n",
      "│    │    └─ReLU: 3-25                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-26                 [128, 128, 16, 16]        147,456\n",
      "│    │    └─BatchNorm2d: 3-27            [128, 128, 16, 16]        256\n",
      "│    │    └─ReLU: 3-28                   [128, 128, 16, 16]        --\n",
      "├─Sequential: 1-6                        [128, 256, 8, 8]          --\n",
      "│    └─ResidualBlock: 2-5                [128, 256, 8, 8]          --\n",
      "│    │    └─Sequential: 3-29             [128, 256, 8, 8]          33,280\n",
      "│    │    └─Conv2d: 3-30                 [128, 256, 8, 8]          294,912\n",
      "│    │    └─BatchNorm2d: 3-31            [128, 256, 8, 8]          512\n",
      "│    │    └─ReLU: 3-32                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-33                 [128, 256, 8, 8]          589,824\n",
      "│    │    └─BatchNorm2d: 3-34            [128, 256, 8, 8]          512\n",
      "│    │    └─ReLU: 3-35                   [128, 256, 8, 8]          --\n",
      "│    └─ResidualBlock: 2-6                [128, 256, 8, 8]          --\n",
      "│    │    └─Identity: 3-36               [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-37                 [128, 256, 8, 8]          589,824\n",
      "│    │    └─BatchNorm2d: 3-38            [128, 256, 8, 8]          512\n",
      "│    │    └─ReLU: 3-39                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-40                 [128, 256, 8, 8]          589,824\n",
      "│    │    └─BatchNorm2d: 3-41            [128, 256, 8, 8]          512\n",
      "│    │    └─ReLU: 3-42                   [128, 256, 8, 8]          --\n",
      "├─Sequential: 1-7                        [128, 512, 4, 4]          --\n",
      "│    └─ResidualBlock: 2-7                [128, 512, 4, 4]          --\n",
      "│    │    └─Sequential: 3-43             [128, 512, 4, 4]          132,096\n",
      "│    │    └─Conv2d: 3-44                 [128, 512, 4, 4]          1,179,648\n",
      "│    │    └─BatchNorm2d: 3-45            [128, 512, 4, 4]          1,024\n",
      "│    │    └─ReLU: 3-46                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-47                 [128, 512, 4, 4]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-48            [128, 512, 4, 4]          1,024\n",
      "│    │    └─ReLU: 3-49                   [128, 512, 4, 4]          --\n",
      "│    └─ResidualBlock: 2-8                [128, 512, 4, 4]          --\n",
      "│    │    └─Identity: 3-50               [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-51                 [128, 512, 4, 4]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-52            [128, 512, 4, 4]          1,024\n",
      "│    │    └─ReLU: 3-53                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-54                 [128, 512, 4, 4]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-55            [128, 512, 4, 4]          1,024\n",
      "│    │    └─ReLU: 3-56                   [128, 512, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-8                 [128, 512, 1, 1]          --\n",
      "├─Linear: 1-9                            [128, 10]                 5,130\n",
      "==========================================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 71.10\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 1258.30\n",
      "Params size (MB): 44.70\n",
      "Estimated Total Size (MB): 1304.57\n",
      "==========================================================================================\n",
      "Validation - Precision: 0.6193, Recall: 0.5083, F1 Score: 0.5031\n",
      "Epoch [1/2], Train Loss: 1.4366, Val Loss: 1.3692, Val Acc: 50.71%\n",
      "New Best Model Saved! Validation Accuracy: 50.71%\n",
      "Validation - Precision: 0.3511, Recall: 0.3390, F1 Score: 0.3275\n",
      "Epoch [2/2], Train Loss: 2.1017, Val Loss: 1.7754, Val Acc: 34.09%\n",
      "Test Accuracy: 51.09%\n",
      "Test Precision: 0.6281, Recall: 0.5109, F1 Score: 0.5064\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchinfo import summary\n",
    "\n",
    "# GPU 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 학습 데이터 증강 설정 (후반 학습용)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (ResNet18)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # 스킵 커넥션 (차원 맞춰주는 역할)\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual Blocks (2개씩 반복)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(64, 64),\n",
    "            ResidualBlock(64, 64)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, stride=2),\n",
    "            ResidualBlock(128, 128)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, stride=2),\n",
    "            ResidualBlock(256, 256)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, stride=2),\n",
    "            ResidualBlock(512, 512)\n",
    "        )\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # 초기 conv\n",
    "\n",
    "        x = self.layer1(x)  # [B, 64, 32, 32]\n",
    "        x = self.layer2(x)  # [B, 128, 16, 16]\n",
    "        x = self.layer3(x)  # [B, 256, 8, 8]\n",
    "        x = self.layer4(x)  # [B, 512, 4, 4]\n",
    "\n",
    "        x = self.global_pool(x)  # [B, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # [B, 512]\n",
    "        x = self.fc(x)  # [B, 10]\n",
    "\n",
    "        return x\n",
    "\n",
    "# 모델 초기화\n",
    "model = CustomResNet18(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# summary 호출\n",
    "print(summary(model, input_size=(BATCH_SIZE, 3, 32, 32)))\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # 반환:(최댓값, 인덱스)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    # load_model()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU9C9STY9odn"
   },
   "source": [
    "## 🧠 Pretrained ResNet18\n",
    "- 🔧 필요한 import 추가\n",
    "\n",
    "```python\n",
    "from torchvision.models import resnet18\n",
    "```\n",
    "\n",
    "- 🧠 모델 생성 함수 만들기 (사전학습 가중치 사용)\n",
    "\n",
    "```python\n",
    "def build_pretrained_resnet18(num_classes=10):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # 선택: CIFAR에 맞게 conv1 수정 (안 해도 동작은 함)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    return model.to(DEVICE)\n",
    "```\n",
    "\n",
    "- 🧠 전이학습 옵션\n",
    "\n",
    "```python\n",
    "# 1. 모델 정의\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "# 2. 출력 레이어 수정 (1000 → 10)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "# 3. 파라미터 학습 여부 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # 전체 동결 (freeze)\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True   # fc만 학습 가능\n",
    "\n",
    "# 4. 모델 디바이스로 이동\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34281,
     "status": "ok",
     "timestamp": 1744041810759,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "cly53L9M9ox8",
    "outputId": "659f2d3c-f726-48a6-c39c-eae4d466be56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 90.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [128, 10]                 --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         (1,728)\n",
      "├─BatchNorm2d: 1-2                       [128, 64, 32, 32]         (128)\n",
      "├─ReLU: 1-3                              [128, 64, 32, 32]         --\n",
      "├─Identity: 1-4                          [128, 64, 32, 32]         --\n",
      "├─Sequential: 1-5                        [128, 64, 32, 32]         --\n",
      "│    └─BasicBlock: 2-1                   [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-2             [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-3                    [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-4                  [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-5             [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-6                    [128, 64, 32, 32]         --\n",
      "│    └─BasicBlock: 2-2                   [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-7                  [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-8             [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-9                    [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-10                 [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-11            [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-12                   [128, 64, 32, 32]         --\n",
      "├─Sequential: 1-6                        [128, 128, 16, 16]        --\n",
      "│    └─BasicBlock: 2-3                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-13                 [128, 128, 16, 16]        (73,728)\n",
      "│    │    └─BatchNorm2d: 3-14            [128, 128, 16, 16]        (256)\n",
      "│    │    └─ReLU: 3-15                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-16                 [128, 128, 16, 16]        (147,456)\n",
      "│    │    └─BatchNorm2d: 3-17            [128, 128, 16, 16]        (256)\n",
      "│    │    └─Sequential: 3-18             [128, 128, 16, 16]        (8,448)\n",
      "│    │    └─ReLU: 3-19                   [128, 128, 16, 16]        --\n",
      "│    └─BasicBlock: 2-4                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-20                 [128, 128, 16, 16]        (147,456)\n",
      "│    │    └─BatchNorm2d: 3-21            [128, 128, 16, 16]        (256)\n",
      "│    │    └─ReLU: 3-22                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-23                 [128, 128, 16, 16]        (147,456)\n",
      "│    │    └─BatchNorm2d: 3-24            [128, 128, 16, 16]        (256)\n",
      "│    │    └─ReLU: 3-25                   [128, 128, 16, 16]        --\n",
      "├─Sequential: 1-7                        [128, 256, 8, 8]          --\n",
      "│    └─BasicBlock: 2-5                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-26                 [128, 256, 8, 8]          (294,912)\n",
      "│    │    └─BatchNorm2d: 3-27            [128, 256, 8, 8]          (512)\n",
      "│    │    └─ReLU: 3-28                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-29                 [128, 256, 8, 8]          (589,824)\n",
      "│    │    └─BatchNorm2d: 3-30            [128, 256, 8, 8]          (512)\n",
      "│    │    └─Sequential: 3-31             [128, 256, 8, 8]          (33,280)\n",
      "│    │    └─ReLU: 3-32                   [128, 256, 8, 8]          --\n",
      "│    └─BasicBlock: 2-6                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-33                 [128, 256, 8, 8]          (589,824)\n",
      "│    │    └─BatchNorm2d: 3-34            [128, 256, 8, 8]          (512)\n",
      "│    │    └─ReLU: 3-35                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-36                 [128, 256, 8, 8]          (589,824)\n",
      "│    │    └─BatchNorm2d: 3-37            [128, 256, 8, 8]          (512)\n",
      "│    │    └─ReLU: 3-38                   [128, 256, 8, 8]          --\n",
      "├─Sequential: 1-8                        [128, 512, 4, 4]          --\n",
      "│    └─BasicBlock: 2-7                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-39                 [128, 512, 4, 4]          (1,179,648)\n",
      "│    │    └─BatchNorm2d: 3-40            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─ReLU: 3-41                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-42                 [128, 512, 4, 4]          (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-43            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─Sequential: 3-44             [128, 512, 4, 4]          (132,096)\n",
      "│    │    └─ReLU: 3-45                   [128, 512, 4, 4]          --\n",
      "│    └─BasicBlock: 2-8                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-46                 [128, 512, 4, 4]          (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-47            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─ReLU: 3-48                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-49                 [128, 512, 4, 4]          (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-50            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─ReLU: 3-51                   [128, 512, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [128, 512, 1, 1]          --\n",
      "├─Linear: 1-10                           [128, 10]                 5,130\n",
      "==========================================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 11,168,832\n",
      "Total mult-adds (Units.GIGABYTES): 71.10\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 1258.30\n",
      "Params size (MB): 44.70\n",
      "Estimated Total Size (MB): 1304.57\n",
      "==========================================================================================\n",
      "Validation - Precision: 0.4768, Recall: 0.4627, F1 Score: 0.4543\n",
      "Epoch [1/2], Train Loss: 1.6909, Val Loss: 1.5390, Val Acc: 46.31%\n",
      "New Best Model Saved! Validation Accuracy: 46.31%\n",
      "Validation - Precision: 0.4888, Recall: 0.4860, F1 Score: 0.4831\n",
      "Epoch [2/2], Train Loss: 1.4657, Val Loss: 1.4825, Val Acc: 48.59%\n",
      "New Best Model Saved! Validation Accuracy: 48.59%\n",
      "Test Accuracy: 48.93%\n",
      "Test Precision: 0.4924, Recall: 0.4893, F1 Score: 0.4859\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchinfo import summary\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# GPU 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 학습 데이터 증강 설정 (후반 학습용)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 불러오기 (ResNet18)\n",
    "def build_pretrained_resnet18(num_classes=10):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # 선택: CIFAR에 맞게 conv1 수정 (안 해도 동작은 함)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    # 파라미터 학습 여부 설정\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # 전체 동결 (freeze)\n",
    "\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True   # fc만 학습 가능\n",
    "\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# 모델 초기화\n",
    "model = build_pretrained_resnet18(num_classes=NUM_CLASSES)\n",
    "\n",
    "# summary 호출\n",
    "print(summary(model, input_size=(BATCH_SIZE, 3, 32, 32)))\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # 반환:(최댓값, 인덱스)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    # load_model()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPRxvItxHbq1p1B07sI2R4c",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
