{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytqiMA2q-lcG"
   },
   "source": [
    "## PYTORCH MLP 모델\n",
    "\n",
    "- CIFAR10 데이터셋을 사용하여 간단한 MLP 모델을 훈련 및 평가하는 내용입니다\n",
    "- 모델은 Fully Connected Layer들로 구성\n",
    "- 활성화 함수로는 ReLU를 사용\n",
    "- 훈련 함수와 테스트 함수도 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281623,
     "status": "ok",
     "timestamp": 1743000474589,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "O937L-sivzUw",
    "outputId": "1a97fd43-8084-448b-d931-b9b20c91634b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:10<00:00, 15.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6538\n",
      "Epoch [2/10], Loss: 1.4355\n",
      "Epoch [3/10], Loss: 1.3241\n",
      "Epoch [4/10], Loss: 1.2271\n",
      "Epoch [5/10], Loss: 1.1404\n",
      "Epoch [6/10], Loss: 1.0678\n",
      "Epoch [7/10], Loss: 0.9954\n",
      "Epoch [8/10], Loss: 0.9235\n",
      "Epoch [9/10], Loss: 0.8585\n",
      "Epoch [10/10], Loss: 0.7944\n",
      "Test Accuracy: 54.05%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "\n",
    "# 데이터 전처리 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsOnCmIF-WQ1"
   },
   "source": [
    "## 📌 **하이퍼파라미터 수정**\n",
    "\n",
    "### Weight Decay (가중치 감쇠)\n",
    "- `weight_decay=0.0001`으로 **Adam 옵티마이저**에 추가했습니다.\n",
    "\n",
    "### Dropout Rate (드롭아웃 확률)\n",
    "- MLP 모델의 각 레이어 사이에 `nn.Dropout(0.5)`를 추가했습니다.\n",
    "\n",
    "### Batch Normalization (배치 정규화)\n",
    "- **레이어마다** `nn.BatchNorm1d()`를 적용했습니다.\n",
    "\n",
    "### Learning Rate Scheduler (학습률 스케줄러)\n",
    "- `torch.optim.lr_scheduler.StepLR`을 이용하여 **매 에폭 5회마다 학습률을 10%로 감소**시키도록 설정했습니다.\n",
    "\n",
    "### Activation Function (활성화 함수)\n",
    "- 마지막 **Hidden Layer**는 `nn.LeakyReLU(0.1)`로 변경했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272512,
     "status": "ok",
     "timestamp": 1743000763460,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "ZMAV6Xqv7KbD",
    "outputId": "607087d9-bbf0-4bdd-e453-074f0ff3ce6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.7861\n",
      "Epoch [2/10], Loss: 1.6285\n",
      "Epoch [3/10], Loss: 1.5698\n",
      "Epoch [4/10], Loss: 1.5375\n",
      "Epoch [5/10], Loss: 1.5103\n",
      "Epoch [6/10], Loss: 1.4344\n",
      "Epoch [7/10], Loss: 1.3983\n",
      "Epoch [8/10], Loss: 1.3852\n",
      "Epoch [9/10], Loss: 1.3641\n",
      "Epoch [10/10], Loss: 1.3533\n",
      "Test Accuracy: 53.97%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001  # 가중치 감쇠\n",
    "DROPOUT_RATE = 0.5  # 드롭아웃 확률\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "\n",
    "# 데이터 전처리 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Op6BHk9cm5"
   },
   "source": [
    "### 📌 레이어 추가 및 하이퍼파라미터 변경\n",
    "\n",
    "### 🔨 레이어 추가\n",
    "- 기존의 모델 구조에 **`nn.Linear(256, 256)`** 레이어를 추가하였습니다.  \n",
    "- 추가된 레이어 이후에도 일관성을 유지하기 위해 **Batch Normalization, ReLU, Dropout**을 추가하였습니다.\n",
    "\n",
    "```python\n",
    "nn.Linear(256, 256),  # 추가된 레이어\n",
    "nn.BatchNorm1d(256),\n",
    "nn.ReLU(),\n",
    "nn.Dropout(DROPOUT_RATE),\n",
    "```\n",
    "\n",
    "### 🔨 하이퍼파라미터 변경\n",
    "- **DROPOUT_RATE**: 0.5 ➔ 0.3 (과적합 방지를 위해 조정)\n",
    "- **WEIGHT_DECAY**: 0.0001 ➔ 1e-5 (모델의 학습 능력 향상을 위해 낮춤)\n",
    "\n",
    "### 🔨 학습률 스케줄러 변경\n",
    "- 학습률 감소 비율을 0.1 ➔ 0.5 로 변경하여 너무 급격히 감소하지 않도록 설정\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308141,
     "status": "ok",
     "timestamp": 1743001381660,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "syeTs__58k5A",
    "outputId": "4f801bfb-d86a-4d42-8b95-1935e2c0a472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.7386\n",
      "Best model saved with loss: 1.7386\n",
      "Epoch [2/10], Loss: 1.5618\n",
      "Best model saved with loss: 1.5618\n",
      "Epoch [3/10], Loss: 1.4865\n",
      "Best model saved with loss: 1.4865\n",
      "Epoch [4/10], Loss: 1.4331\n",
      "Best model saved with loss: 1.4331\n",
      "Epoch [5/10], Loss: 1.3920\n",
      "Best model saved with loss: 1.3920\n",
      "Epoch [6/10], Loss: 1.3116\n",
      "Best model saved with loss: 1.3116\n",
      "Epoch [7/10], Loss: 1.2709\n",
      "Best model saved with loss: 1.2709\n",
      "Epoch [8/10], Loss: 1.2503\n",
      "Best model saved with loss: 1.2503\n",
      "Epoch [9/10], Loss: 1.2269\n",
      "Best model saved with loss: 1.2269\n",
      "Epoch [10/10], Loss: 1.1999\n",
      "Best model saved with loss: 1.1999\n",
      "Test Accuracy: 56.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.pth'  # 모델 저장 경로\n",
    "\n",
    "# 데이터 전처리 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # 추가된 레이어\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),  # LeakyReLU 대신 ReLU 사용\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 스케줄러 변경\n",
    "\n",
    "best_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "\n",
    "def train():\n",
    "    global best_loss\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 모델 저장: 베스트 로스일 경우\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp12G_-m_Xoh"
   },
   "source": [
    "### 📌 **데이터 증강 기법 적용**\n",
    "- `RandomHorizontalFlip()`: 이미지를 좌우로 무작위로 뒤집음.\n",
    "- `RandomCrop(32, padding=4)`: 원래 크기로 되돌리기 전에 패딩을 추가하여 임의의 부분을 잘라냄.\n",
    "- `ColorJitter()`: 밝기, 대비, 채도, 색조를 무작위로 변경.\n",
    "- `RandomRotation(10)`: 이미지를 최대 10도 범위에서 무작위로 회전.\n",
    "\n",
    "```python\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "```\n",
    "\n",
    "- 데이터 증강은 데이터의 수를 늘리는 것이 아니라, 학습 과정에서 기존 데이터를 변형하여 다양한 방식으로 보여주는 것입니다.\n",
    "\n",
    "### 📌 **데이터 증강의 특징**\n",
    "- 데이터 수는 그대로 유지됩니다. (`train_dataset`의 크기는 동일합니다.)\n",
    "- 매번 미니배치가 모델에 전달될 때, 동적으로 변형이 적용됩니다.\n",
    "- 같은 이미지라 하더라도 매번 다른 형태(회전, 색상 변경, 좌우 반전 등)로 모델에 입력됩니다.\n",
    "- 증강된 이미지는 메모리에 저장되지 않고, 매번 실시간으로 처리됩니다.\n",
    "\n",
    "### 📌 **예시**\n",
    "- 학습 데이터셋의 크기는 여전히 `len(train_dataset) = 50,000` 입니다.\n",
    "- 하지만 각 에포크마다 모델은 완전히 다른 형태의 변형된 이미지를 보게 됩니다.\n",
    "- 결과적으로는 데이터의 다양성을 늘려 과적합을 방지하고 모델의 일반화 성능을 향상시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 692621,
     "status": "ok",
     "timestamp": 1743002300480,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "7FtPwtXT_ch8",
    "outputId": "4a32743c-bb37-444c-a2fe-f7730cf732f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.9366\n",
      "Best model saved with loss: 1.9366\n",
      "Epoch [2/10], Loss: 1.8013\n",
      "Best model saved with loss: 1.8013\n",
      "Epoch [3/10], Loss: 1.7497\n",
      "Best model saved with loss: 1.7497\n",
      "Epoch [4/10], Loss: 1.7116\n",
      "Best model saved with loss: 1.7116\n",
      "Epoch [5/10], Loss: 1.6841\n",
      "Best model saved with loss: 1.6841\n",
      "Epoch [6/10], Loss: 1.6448\n",
      "Best model saved with loss: 1.6448\n",
      "Epoch [7/10], Loss: 1.6258\n",
      "Best model saved with loss: 1.6258\n",
      "Epoch [8/10], Loss: 1.6110\n",
      "Best model saved with loss: 1.6110\n",
      "Epoch [9/10], Loss: 1.6040\n",
      "Best model saved with loss: 1.6040\n",
      "Epoch [10/10], Loss: 1.5969\n",
      "Best model saved with loss: 1.5969\n",
      "Test Accuracy: 50.23%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.pth'  # 모델 저장 경로\n",
    "\n",
    "# 데이터 전처리 정의 (데이터 증강 추가)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # 추가된 레이어\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 스케줄러 변경\n",
    "\n",
    "best_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "\n",
    "def train():\n",
    "    global best_loss\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 모델 저장: 베스트 로스일 경우\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjhq5d-PDZ26"
   },
   "source": [
    "### 📌 **데이터 증강 강도 변경**\n",
    "\n",
    "1. 데이터 증강 강도 감소:\n",
    "- ColorJitter: brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05 로 줄임.\n",
    "- RandomRotation: 10 ➔ 5 로 줄임.\n",
    "\n",
    "2. 학습 에폭 증가:\n",
    "- EPOCHS = 10 ➔ 20 으로 증가.\n",
    "\n",
    "3. 두 가지 데이터 증강 기법 정의:\n",
    "- weak_transform: 초기 학습에 사용, 가벼운 데이터 증강 (RandomHorizontalFlip, RandomCrop 만 적용)\n",
    "- strong_transform: 후반 학습에 사용, 강한 데이터 증강 (ColorJitter, RandomRotation 추가)\n",
    "\n",
    "4. 에폭의 절반 이후 데이터 증강 기법 교체:\n",
    "\n",
    "```python\n",
    "if epoch == EPOCHS // 2:\n",
    "    train_dataset.transform = strong_transform\n",
    "    print(\"Switched to Strong Data Augmentation\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GhlQcaOD9RB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # 에폭 수 증가\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.pth'  # 모델 저장 경로\n",
    "\n",
    "# CIFAR-10 데이터셋의 실제 평균과 표준편차 사용\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드 (초기에는 약한 데이터 증강 사용)\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # 추가된 레이어\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 스케줄러 변경\n",
    "\n",
    "best_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "\n",
    "def train():\n",
    "    global best_loss, train_dataset\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 에폭 절반 이후 강한 데이터 증강으로 교체\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.transform = strong_transform\n",
    "            print(\"Switched to Strong Data Augmentation\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 모델 저장: 베스트 로스일 경우\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ubl4ZTcAusv"
   },
   "source": [
    "### 📌 **CIFAR10 정규화값 변경**\n",
    "\n",
    "- CIFAR-10의 평균: (0.4914, 0.4822, 0.4465)\n",
    "\n",
    "- CIFAR-10의 표준편차: (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "```python\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 678573,
     "status": "ok",
     "timestamp": 1743003000562,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "9L2t74lpByvJ",
    "outputId": "fde3f18e-5212-4199-fff5-d973626498c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.9369\n",
      "Best model saved with loss: 1.9369\n",
      "Epoch [2/10], Loss: 1.8046\n",
      "Best model saved with loss: 1.8046\n",
      "Epoch [3/10], Loss: 1.7432\n",
      "Best model saved with loss: 1.7432\n",
      "Epoch [4/10], Loss: 1.7115\n",
      "Best model saved with loss: 1.7115\n",
      "Epoch [5/10], Loss: 1.6846\n",
      "Best model saved with loss: 1.6846\n",
      "Epoch [6/10], Loss: 1.6456\n",
      "Best model saved with loss: 1.6456\n",
      "Epoch [7/10], Loss: 1.6194\n",
      "Best model saved with loss: 1.6194\n",
      "Epoch [8/10], Loss: 1.6148\n",
      "Best model saved with loss: 1.6148\n",
      "Epoch [9/10], Loss: 1.6003\n",
      "Best model saved with loss: 1.6003\n",
      "Epoch [10/10], Loss: 1.5886\n",
      "Best model saved with loss: 1.5886\n",
      "Test Accuracy: 49.81%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.pth'  # 모델 저장 경로\n",
    "\n",
    "# CIFAR-10 데이터셋의 실제 평균과 표준편차 사용\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 데이터 전처리 정의 (데이터 증강 추가)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # 추가된 레이어\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 스케줄러 변경\n",
    "\n",
    "best_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "\n",
    "def train():\n",
    "    global best_loss\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 모델 저장: 베스트 로스일 경우\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vILZbWQ0E4LZ"
   },
   "source": [
    "###📌 **모델 불러오기 기능 추가**\n",
    "\n",
    "1. 모델 불러오기 함수 (load_model) 추가:\n",
    "\n",
    "```python\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "```\n",
    "- 학습을 시작하기 전에 저장된 모델(`best_model.pth`)이 존재하면 이를 불러옵니다.\n",
    "- 모델이 없으면 새로 학습을 시작합니다.\n",
    "\n",
    "2. 모델 불러오기 코드 추가 (`main()` 함수 내):\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()  # 저장된 모델 불러오기\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 802729,
     "status": "ok",
     "timestamp": 1743077016996,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "4kItpKG1FMZZ",
    "outputId": "43a7b8b6-dee3-48fe-9afa-58ba3241217a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:01<00:00, 86.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n",
      "Epoch [1/20], Loss: 1.8899\n",
      "Best model saved with loss: 1.8899\n",
      "Epoch [2/20], Loss: 1.7429\n",
      "Best model saved with loss: 1.7429\n",
      "Epoch [3/20], Loss: 1.6875\n",
      "Best model saved with loss: 1.6875\n",
      "Epoch [4/20], Loss: 1.6513\n",
      "Best model saved with loss: 1.6513\n",
      "Epoch [5/20], Loss: 1.6220\n",
      "Best model saved with loss: 1.6220\n",
      "Epoch [6/20], Loss: 1.5676\n",
      "Best model saved with loss: 1.5676\n",
      "Epoch [7/20], Loss: 1.5471\n",
      "Best model saved with loss: 1.5471\n",
      "Epoch [8/20], Loss: 1.5373\n",
      "Best model saved with loss: 1.5373\n",
      "Epoch [9/20], Loss: 1.5266\n",
      "Best model saved with loss: 1.5266\n",
      "Epoch [10/20], Loss: 1.5135\n",
      "Best model saved with loss: 1.5135\n",
      "Switched to Strong Data Augmentation\n",
      "Epoch [11/20], Loss: 1.5187\n",
      "Epoch [12/20], Loss: 1.5073\n",
      "Best model saved with loss: 1.5073\n",
      "Epoch [13/20], Loss: 1.5022\n",
      "Best model saved with loss: 1.5022\n",
      "Epoch [14/20], Loss: 1.4918\n",
      "Best model saved with loss: 1.4918\n",
      "Epoch [15/20], Loss: 1.4858\n",
      "Best model saved with loss: 1.4858\n",
      "Epoch [16/20], Loss: 1.4766\n",
      "Best model saved with loss: 1.4766\n",
      "Epoch [17/20], Loss: 1.4661\n",
      "Best model saved with loss: 1.4661\n",
      "Epoch [18/20], Loss: 1.4628\n",
      "Best model saved with loss: 1.4628\n",
      "Epoch [19/20], Loss: 1.4653\n",
      "Epoch [20/20], Loss: 1.4563\n",
      "Best model saved with loss: 1.4563\n",
      "Test Accuracy: 53.96%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # 에폭 수 증가\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.pth'  # 모델 저장 경로\n",
    "\n",
    "# CIFAR-10 데이터셋의 실제 평균과 표준편차 사용\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드 (초기에는 약한 데이터 증강 사용)\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # 추가된 레이어\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 스케줄러 변경\n",
    "\n",
    "best_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "\n",
    "def train():\n",
    "    global best_loss, train_dataset\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 에폭 절반 이후 강한 데이터 증강으로 교체\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.transform = strong_transform\n",
    "            print(\"Switched to Strong Data Augmentation\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 모델 저장: 베스트 로스일 경우\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()  # 저장된 모델 불러오기\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi0MuTNNGH_2"
   },
   "source": [
    "###📌 **모델 시각화**\n",
    "\n",
    "1. torchviz 라이브러리 사용:\n",
    "- 모델의 구조를 시각화하여 MLP_Model_Structure.png 로 저장합니다.\n",
    "\n",
    "2. 모델 시각화 함수 (visualize_model) 추가:\n",
    "\n",
    "```python\n",
    "def visualize_model():\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)  # CIFAR10 이미지 사이즈\n",
    "    output = model(dummy_input)\n",
    "    graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "    graph.render(\"MLP_Model_Structure\", format=\"png\", cleanup=True)\n",
    "    print(\"Model visualization saved as 'MLP_Model_Structure.png'\")\n",
    "```\n",
    "\n",
    "3. 모델 시각화 함수 호출:\n",
    "\n",
    "```python\n",
    "visualize_model()  # 모델 시각화 호출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FbNCtAGJdm"
   },
   "outputs": [],
   "source": [
    "# !pip install torchviz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torchviz import make_dot  # 모델 시각화를 위한 라이브러리 추가\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # 에폭 수 증가\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.pth'  # 모델 저장 경로\n",
    "\n",
    "# CIFAR-10 데이터셋의 실제 평균과 표준편차 사용\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드 (초기에는 약한 데이터 증강 사용)\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # 추가된 레이어\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# 모델 시각화 함수 정의\n",
    "\n",
    "def visualize_model():\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)  # CIFAR10 이미지 사이즈\n",
    "    output = model(dummy_input)\n",
    "    graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "    graph.render(\"MLP_Model_Structure\", format=\"png\", cleanup=True)\n",
    "    print(\"Model visualization saved as 'MLP_Model_Structure.png'\")\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 스케줄러 변경\n",
    "\n",
    "best_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "\n",
    "def train():\n",
    "    global best_loss, train_dataset\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.transform = strong_transform\n",
    "            print(\"Switched to Strong Data Augmentation\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()\n",
    "    visualize_model()  # 모델 시각화 호출\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q0EcBrQL4kD"
   },
   "source": [
    "###📌 **Train/Validation/Test 데이터셋 활**\n",
    "\n",
    "1. 학습 데이터와 검증 데이터를 분리하기 (Train-Validation Split)\n",
    "- 데이터셋 분할을 위한 `random_split()` 사용\n",
    "- 하이퍼파라미터에 `SPLIT_RATIO`를 설정하여 학습 데이터와 검증 데이터 분리 비율 설정\n",
    "\n",
    "2. 학습 코드 수정하기 (Train & Validation)\n",
    "- 기존의 `train()` 함수에서 학습 후 매 에폭마다 검증 데이터를 평가하는 코드를 추가\n",
    "\n",
    "\n",
    "3. 검증 함수 추가하기\n",
    "- `validate()` 함수\n",
    "\n",
    "```python\n",
    "def validate():\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    model.train()  # 다시 학습 모드로 전환\n",
    "    return avg_loss, accuracy\n",
    "```\n",
    "\n",
    "4. Metric 추가\n",
    "- **Precision**, **Recall**, **F1-Score** 추가 (매 에폭마다 검증 시 계산)\n",
    "\n",
    "5. Early Stopping 수정\n",
    "- Validation Accuracy(`best_val_acc`)가 개선되지 않으면, `patience`가 3일 때 조기 종료."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334606,
     "status": "ok",
     "timestamp": 1743079163248,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "OjAIovzcjeOX",
    "outputId": "fbdfd395-47e6-4451-e5a5-ec3c2b9b894b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4376, Recall: 0.4379, F1 Score: 0.4288\n",
      "Epoch [1/20], Train Loss: 1.7689, Val Loss: 1.2486, Val Acc: 43.73%\n",
      "New Best Model Saved! Validation Accuracy: 43.73%\n",
      "Validation - Precision: 0.4723, Recall: 0.4722, F1 Score: 0.4660\n",
      "Epoch [2/20], Train Loss: 1.5828, Val Loss: 1.1320, Val Acc: 47.11%\n",
      "New Best Model Saved! Validation Accuracy: 47.11%\n",
      "Validation - Precision: 0.4895, Recall: 0.4881, F1 Score: 0.4818\n",
      "Epoch [3/20], Train Loss: 1.5040, Val Loss: 1.1204, Val Acc: 48.70%\n",
      "New Best Model Saved! Validation Accuracy: 48.70%\n",
      "Validation - Precision: 0.4985, Recall: 0.4995, F1 Score: 0.4946\n",
      "Epoch [4/20], Train Loss: 1.4524, Val Loss: 1.1050, Val Acc: 49.96%\n",
      "New Best Model Saved! Validation Accuracy: 49.96%\n",
      "Validation - Precision: 0.5107, Recall: 0.5117, F1 Score: 0.5066\n",
      "Epoch [5/20], Train Loss: 1.4065, Val Loss: 1.1460, Val Acc: 51.13%\n",
      "New Best Model Saved! Validation Accuracy: 51.13%\n",
      "Validation - Precision: 0.5234, Recall: 0.5263, F1 Score: 0.5211\n",
      "Epoch [6/20], Train Loss: 1.3189, Val Loss: 1.0692, Val Acc: 52.57%\n",
      "New Best Model Saved! Validation Accuracy: 52.57%\n",
      "Validation - Precision: 0.5257, Recall: 0.5292, F1 Score: 0.5260\n",
      "Epoch [7/20], Train Loss: 1.2895, Val Loss: 0.8917, Val Acc: 52.91%\n",
      "New Best Model Saved! Validation Accuracy: 52.91%\n",
      "Validation - Precision: 0.5398, Recall: 0.5383, F1 Score: 0.5360\n",
      "Epoch [8/20], Train Loss: 1.2538, Val Loss: 1.0956, Val Acc: 53.78%\n",
      "New Best Model Saved! Validation Accuracy: 53.78%\n",
      "Validation - Precision: 0.5383, Recall: 0.5394, F1 Score: 0.5355\n",
      "Epoch [9/20], Train Loss: 1.2374, Val Loss: 1.0138, Val Acc: 53.98%\n",
      "New Best Model Saved! Validation Accuracy: 53.98%\n",
      "Validation - Precision: 0.5436, Recall: 0.5476, F1 Score: 0.5444\n",
      "Epoch [10/20], Train Loss: 1.2088, Val Loss: 1.0396, Val Acc: 54.74%\n",
      "New Best Model Saved! Validation Accuracy: 54.74%\n",
      "Validation - Precision: 0.4475, Recall: 0.4566, F1 Score: 0.4477\n",
      "Epoch [11/20], Train Loss: 1.6474, Val Loss: 1.2091, Val Acc: 45.62%\n",
      "Validation - Precision: 0.4612, Recall: 0.4659, F1 Score: 0.4601\n",
      "Epoch [12/20], Train Loss: 1.5805, Val Loss: 1.2213, Val Acc: 46.52%\n",
      "Validation - Precision: 0.4749, Recall: 0.4763, F1 Score: 0.4718\n",
      "Epoch [13/20], Train Loss: 1.5554, Val Loss: 1.2802, Val Acc: 47.62%\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 55.46%\n",
      "Test Precision: 0.5502, Recall: 0.5546, F1 Score: 0.5511\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 설정 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 설정 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 설정\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # 에폭 절반 이후 강한 데이터 증강으로 전환\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()  # 모델 로드 시도\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXNRlD6fpeDB"
   },
   "source": [
    "###📌 **하이퍼 파라미터 서치**\n",
    "\n",
    "1. **Optuna** 사용\n",
    "- `import optuna`\n",
    "\n",
    "2. 하이퍼파라미터 탐색 대상:\n",
    "\n",
    "- `batch_size` (32, 64, 128)\n",
    "- `learning_rate` (1e-4 ~ 1e-2)\n",
    "- `dropout_rate` (0.1 ~ 0.5)\n",
    "- `optimizer` (Adam, SGD)\n",
    "\n",
    "3. 학습 중간 검증:\n",
    "- 매 에폭마다 `trial.report()` 로 결과를 보고합니다.\n",
    "- 성능이 개선되지 않으면 `Optuna`가 학습을 조기 중단 (`TrialPruned()`)합니다.\n",
    "\n",
    "4. 학습 완료 후 최적 하이퍼파라미터로 모델 학습 및 테스트 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCrdKDF_qYAT",
    "outputId": "75c9064f-6c10-4bc0-e608-d6f33e5fc20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:06:13,870] A new study created in memory with name: no-name-1dab4e45-5ddc-423f-8608-eb1c86f15e22\n",
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4291, Recall: 0.4247, F1 Score: 0.4202\n",
      "Validation - Precision: 0.4652, Recall: 0.4653, F1 Score: 0.4588\n",
      "Validation - Precision: 0.4857, Recall: 0.4858, F1 Score: 0.4804\n",
      "Validation - Precision: 0.5000, Recall: 0.5026, F1 Score: 0.4986\n",
      "Validation - Precision: 0.5125, Recall: 0.5137, F1 Score: 0.5079\n",
      "Validation - Precision: 0.5212, Recall: 0.5170, F1 Score: 0.5121\n",
      "Validation - Precision: 0.5296, Recall: 0.5266, F1 Score: 0.5233\n",
      "Validation - Precision: 0.5332, Recall: 0.5337, F1 Score: 0.5314\n",
      "Validation - Precision: 0.5402, Recall: 0.5411, F1 Score: 0.5374\n",
      "Validation - Precision: 0.5507, Recall: 0.5502, F1 Score: 0.5489\n",
      "Validation - Precision: 0.4498, Recall: 0.4479, F1 Score: 0.4442\n",
      "Validation - Precision: 0.4583, Recall: 0.4601, F1 Score: 0.4561\n",
      "Validation - Precision: 0.4635, Recall: 0.4647, F1 Score: 0.4579\n",
      "Validation - Precision: 0.4687, Recall: 0.4703, F1 Score: 0.4680\n",
      "Validation - Precision: 0.4721, Recall: 0.4755, F1 Score: 0.4699\n",
      "Validation - Precision: 0.4848, Recall: 0.4843, F1 Score: 0.4813\n",
      "Validation - Precision: 0.4763, Recall: 0.4802, F1 Score: 0.4761\n",
      "Validation - Precision: 0.4834, Recall: 0.4852, F1 Score: 0.4802\n",
      "Validation - Precision: 0.4840, Recall: 0.4868, F1 Score: 0.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:16:47,473] Trial 0 finished with value: 55.21 and parameters: {'batch_size': 128, 'learning_rate': 0.003109548697581316, 'dropout_rate': 0.22553735533002361, 'weight_decay': 4.508346108254859e-05, 'step_size': 8, 'gamma': 0.7972745415846182, 'optimizer': 'SGD'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4942, Recall: 0.4965, F1 Score: 0.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3357, Recall: 0.3373, F1 Score: 0.3296\n",
      "Validation - Precision: 0.3541, Recall: 0.3517, F1 Score: 0.3443\n",
      "Validation - Precision: 0.3536, Recall: 0.3549, F1 Score: 0.3410\n",
      "Validation - Precision: 0.3732, Recall: 0.3754, F1 Score: 0.3716\n",
      "Validation - Precision: 0.3756, Recall: 0.3697, F1 Score: 0.3517\n",
      "Validation - Precision: 0.3895, Recall: 0.3808, F1 Score: 0.3714\n",
      "Validation - Precision: 0.3818, Recall: 0.3759, F1 Score: 0.3666\n",
      "Validation - Precision: 0.4002, Recall: 0.3938, F1 Score: 0.3762\n",
      "Validation - Precision: 0.3987, Recall: 0.4015, F1 Score: 0.3905\n",
      "Validation - Precision: 0.4091, Recall: 0.4067, F1 Score: 0.3962\n",
      "Validation - Precision: 0.4096, Recall: 0.4087, F1 Score: 0.4062\n",
      "Validation - Precision: 0.3924, Recall: 0.3981, F1 Score: 0.3859\n",
      "Validation - Precision: 0.4088, Recall: 0.4097, F1 Score: 0.4038\n",
      "Validation - Precision: 0.4191, Recall: 0.4238, F1 Score: 0.4139\n",
      "Validation - Precision: 0.4349, Recall: 0.4380, F1 Score: 0.4301\n",
      "Validation - Precision: 0.4375, Recall: 0.4352, F1 Score: 0.4338\n",
      "Validation - Precision: 0.4375, Recall: 0.4387, F1 Score: 0.4345\n",
      "Validation - Precision: 0.4482, Recall: 0.4443, F1 Score: 0.4370\n",
      "Validation - Precision: 0.4346, Recall: 0.4357, F1 Score: 0.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:44:19,385] Trial 1 finished with value: 44.69 and parameters: {'batch_size': 64, 'learning_rate': 0.009320346498472817, 'dropout_rate': 0.237750765520911, 'weight_decay': 2.028978770068843e-05, 'step_size': 7, 'gamma': 0.6376061337271416, 'optimizer': 'Adam'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4314, Recall: 0.4368, F1 Score: 0.4292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3170, Recall: 0.3114, F1 Score: 0.3015\n",
      "Validation - Precision: 0.3439, Recall: 0.3416, F1 Score: 0.3344\n",
      "Validation - Precision: 0.3631, Recall: 0.3622, F1 Score: 0.3533\n",
      "Validation - Precision: 0.3802, Recall: 0.3757, F1 Score: 0.3709\n",
      "Validation - Precision: 0.3875, Recall: 0.3836, F1 Score: 0.3785\n",
      "Validation - Precision: 0.3950, Recall: 0.3926, F1 Score: 0.3880\n",
      "Validation - Precision: 0.4021, Recall: 0.3992, F1 Score: 0.3923\n",
      "Validation - Precision: 0.4083, Recall: 0.4060, F1 Score: 0.3997\n",
      "Validation - Precision: 0.4039, Recall: 0.4036, F1 Score: 0.3977\n",
      "Validation - Precision: 0.4176, Recall: 0.4152, F1 Score: 0.4082\n",
      "Validation - Precision: 0.4155, Recall: 0.4152, F1 Score: 0.4100\n",
      "Validation - Precision: 0.4122, Recall: 0.4117, F1 Score: 0.4053\n",
      "Validation - Precision: 0.4111, Recall: 0.4137, F1 Score: 0.4070\n",
      "Validation - Precision: 0.4256, Recall: 0.4245, F1 Score: 0.4186\n",
      "Validation - Precision: 0.4210, Recall: 0.4188, F1 Score: 0.4129\n",
      "Validation - Precision: 0.4181, Recall: 0.4188, F1 Score: 0.4122\n",
      "Validation - Precision: 0.4198, Recall: 0.4199, F1 Score: 0.4146\n",
      "Validation - Precision: 0.4219, Recall: 0.4231, F1 Score: 0.4175\n",
      "Validation - Precision: 0.4221, Recall: 0.4241, F1 Score: 0.4180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 14:01:31,641] Trial 2 finished with value: 42.63 and parameters: {'batch_size': 64, 'learning_rate': 0.0003805663988899745, 'dropout_rate': 0.2219431290557982, 'weight_decay': 1.3523658748252338e-06, 'step_size': 4, 'gamma': 0.507303455610086, 'optimizer': 'SGD'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4159, Recall: 0.4141, F1 Score: 0.4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3738, Recall: 0.3595, F1 Score: 0.3487\n",
      "Validation - Precision: 0.3874, Recall: 0.3855, F1 Score: 0.3757\n",
      "Validation - Precision: 0.4074, Recall: 0.4017, F1 Score: 0.3967\n",
      "Validation - Precision: 0.3946, Recall: 0.3958, F1 Score: 0.3834\n",
      "Validation - Precision: 0.4224, Recall: 0.4182, F1 Score: 0.4150\n",
      "Validation - Precision: 0.4203, Recall: 0.4176, F1 Score: 0.4092\n",
      "Validation - Precision: 0.4107, Recall: 0.4126, F1 Score: 0.4025\n",
      "Validation - Precision: 0.4166, Recall: 0.4203, F1 Score: 0.4137\n",
      "Validation - Precision: 0.4206, Recall: 0.4258, F1 Score: 0.4174\n",
      "Validation - Precision: 0.4574, Recall: 0.4597, F1 Score: 0.4551\n",
      "Validation - Precision: 0.4556, Recall: 0.4621, F1 Score: 0.4565\n",
      "Validation - Precision: 0.4621, Recall: 0.4650, F1 Score: 0.4610\n",
      "Validation - Precision: 0.4657, Recall: 0.4708, F1 Score: 0.4654\n",
      "Validation - Precision: 0.4629, Recall: 0.4665, F1 Score: 0.4633\n",
      "Validation - Precision: 0.4643, Recall: 0.4703, F1 Score: 0.4641\n",
      "Validation - Precision: 0.4671, Recall: 0.4744, F1 Score: 0.4668\n",
      "Validation - Precision: 0.4742, Recall: 0.4768, F1 Score: 0.4722\n",
      "Validation - Precision: 0.4746, Recall: 0.4791, F1 Score: 0.4748\n",
      "Validation - Precision: 0.4854, Recall: 0.4905, F1 Score: 0.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 14:24:14,852] Trial 3 finished with value: 50.38 and parameters: {'batch_size': 64, 'learning_rate': 0.0012269573803510066, 'dropout_rate': 0.1557565882173503, 'weight_decay': 0.0004650420504685621, 'step_size': 9, 'gamma': 0.35019103938049145, 'optimizer': 'Adam'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4983, Recall: 0.5020, F1 Score: 0.4984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3184, Recall: 0.2817, F1 Score: 0.2509\n",
      "Validation - Precision: 0.3304, Recall: 0.3233, F1 Score: 0.3040\n",
      "Validation - Precision: 0.3445, Recall: 0.3352, F1 Score: 0.3180\n",
      "Validation - Precision: 0.3631, Recall: 0.3532, F1 Score: 0.3407\n",
      "Validation - Precision: 0.3672, Recall: 0.3584, F1 Score: 0.3473\n",
      "Validation - Precision: 0.3848, Recall: 0.3793, F1 Score: 0.3675\n",
      "Validation - Precision: 0.3773, Recall: 0.3785, F1 Score: 0.3618\n",
      "Validation - Precision: 0.3929, Recall: 0.3902, F1 Score: 0.3785\n",
      "Validation - Precision: 0.3988, Recall: 0.3944, F1 Score: 0.3826\n",
      "Validation - Precision: 0.4029, Recall: 0.3991, F1 Score: 0.3866\n",
      "Validation - Precision: 0.4037, Recall: 0.4035, F1 Score: 0.3914\n",
      "Validation - Precision: 0.4064, Recall: 0.4032, F1 Score: 0.3930\n",
      "Validation - Precision: 0.4123, Recall: 0.4073, F1 Score: 0.3965\n",
      "Validation - Precision: 0.3994, Recall: 0.3976, F1 Score: 0.3851\n"
     ]
    }
   ],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import optuna  # 하이퍼파라미터 서치 라이브러리\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 설정 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 설정 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# 하이퍼파라미터 서치\n",
    "def objective(trial):\n",
    "    global model, optimizer, scheduler, BATCH_SIZE, LEARNING_RATE, DROPOUT_RATE, WEIGHT_DECAY, best_val_acc, patience_counter\n",
    "\n",
    "    # 하이퍼파라미터 샘플링\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    model = MLP()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 학습 과정\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validate()  # 검증 단계\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # 에폭 절반 이후 강한 데이터 증강으로 전환\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f\"Best Trial: {study.best_trial.value}\")\n",
    "    print(f\"Best Hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "    # 최적의 하이퍼파라미터로 다시 학습 및 테스트 진행\n",
    "    load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUJf3vCT46vX"
   },
   "source": [
    "###📌 **GPU 자원 활용**\n",
    "\n",
    "1. GPU 설정\n",
    "\n",
    "```python\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "\n",
    "2. 모델 초기화 후 GPU로 이동(`model.to(DEVICE)`)\n",
    "\n",
    "```python\n",
    "model = MLP().to(DEVICE)\n",
    "```\n",
    "\n",
    "3. 입력 데이터 (inputs, labels)도 GPU로 전송(inputs.to(DEVICE)`)\n",
    "\n",
    "```python\n",
    "inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "```\n",
    "\n",
    "4. 손실 함수 (criterion)도 GPU에서 사용하도록 전송\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "```\n",
    "\n",
    "5. 하이퍼파라미터 서치 파트 안에도 똑같이 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvPetMoI5Bz8"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "import optuna  # 하이퍼파라미터 서치 라이브러리\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# GPU 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 설정 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 설정 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        model.to(DEVICE)\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# 하이퍼파라미터 서치\n",
    "def objective(trial):\n",
    "    global model, optimizer, scheduler, BATCH_SIZE, LEARNING_RATE, DROPOUT_RATE, WEIGHT_DECAY, best_val_acc, patience_counter\n",
    "\n",
    "    # 하이퍼파라미터 샘플링\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    model = MLP().to(DEVICE)\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 학습 과정\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validate()  # 검증 단계\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # 에폭 절반 이후 강한 데이터 증강으로 전환\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(study, filename=\"best_study_info.json\"):\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_value\": best_value,\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f\"Best Trial: {study.best_trial.value}\")\n",
    "    print(f\"Best Hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "    # 최적의 하이퍼파라미터로 다시 학습 및 테스트 진행\n",
    "    load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info(study)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAxcHNNQvZgg"
   },
   "source": [
    "###📌 **학습에 필요한 모든 정보 저장**\n",
    "\n",
    "1. 최적 하이퍼파라미터 정보 저장\n",
    "- 최적의 하이퍼파라미터 (`best_params`)\n",
    "- 최적의 검증 정확도 (`best_value`)\n",
    "\n",
    "2. 파이썬 버전 저장\n",
    "\n",
    "- `python_version`\n",
    "\n",
    "3. 플랫폼 정보 저장\n",
    "- `platform_info`\n",
    "\n",
    "4. 설치된 모든 라이브러리 및 버전 정보\n",
    "- `installed_packages`\n",
    "\n",
    "5. 저장 형식: JSON 파일\n",
    "- `import json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUXsEOQvZtl"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "import optuna  # 하이퍼파라미터 서치 라이브러리\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 약한 데이터 증강 설정 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 설정 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# 하이퍼파라미터 서치\n",
    "def objective(trial):\n",
    "    global model, optimizer, scheduler, BATCH_SIZE, LEARNING_RATE, DROPOUT_RATE, WEIGHT_DECAY, best_val_acc, patience_counter\n",
    "\n",
    "    # 하이퍼파라미터 샘플링\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    model = MLP()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 학습 과정\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validate()  # 검증 단계\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # 에폭 절반 이후 강한 데이터 증강으로 전환\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(study, filename=\"best_study_info.json\"):\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_value\": best_value,\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f\"Best Trial: {study.best_trial.value}\")\n",
    "    print(f\"Best Hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "    # 최적의 하이퍼파라미터로 다시 학습 및 테스트 진행\n",
    "    load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245640,
     "status": "ok",
     "timestamp": 1743688571439,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "VPWFnKCS48fB",
    "outputId": "72b193c5-7e13-43dd-d627-5464505d83c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4357, Recall: 0.4322, F1 Score: 0.4309\n",
      "Epoch [1/20], Train Loss: 1.7769, Val Loss: 1.7876, Val Acc: 43.24%\n",
      "New Best Model Saved! Validation Accuracy: 43.24%\n",
      "Validation - Precision: 0.4652, Recall: 0.4618, F1 Score: 0.4577\n",
      "Epoch [2/20], Train Loss: 1.5758, Val Loss: 1.5997, Val Acc: 46.19%\n",
      "New Best Model Saved! Validation Accuracy: 46.19%\n",
      "Validation - Precision: 0.4862, Recall: 0.4863, F1 Score: 0.4796\n",
      "Epoch [3/20], Train Loss: 1.4930, Val Loss: 1.5601, Val Acc: 48.70%\n",
      "New Best Model Saved! Validation Accuracy: 48.70%\n",
      "Validation - Precision: 0.4958, Recall: 0.4915, F1 Score: 0.4877\n",
      "Epoch [4/20], Train Loss: 1.4359, Val Loss: 1.3529, Val Acc: 49.17%\n",
      "New Best Model Saved! Validation Accuracy: 49.17%\n",
      "Validation - Precision: 0.5065, Recall: 0.5065, F1 Score: 0.5027\n",
      "Epoch [5/20], Train Loss: 1.3889, Val Loss: 1.2718, Val Acc: 50.73%\n",
      "New Best Model Saved! Validation Accuracy: 50.73%\n",
      "Validation - Precision: 0.5260, Recall: 0.5231, F1 Score: 0.5205\n",
      "Epoch [6/20], Train Loss: 1.3067, Val Loss: 1.2284, Val Acc: 52.35%\n",
      "New Best Model Saved! Validation Accuracy: 52.35%\n",
      "Validation - Precision: 0.5279, Recall: 0.5339, F1 Score: 0.5288\n",
      "Epoch [7/20], Train Loss: 1.2685, Val Loss: 1.3184, Val Acc: 53.43%\n",
      "New Best Model Saved! Validation Accuracy: 53.43%\n",
      "Validation - Precision: 0.5399, Recall: 0.5405, F1 Score: 0.5385\n",
      "Epoch [8/20], Train Loss: 1.2436, Val Loss: 1.2941, Val Acc: 54.05%\n",
      "New Best Model Saved! Validation Accuracy: 54.05%\n",
      "Validation - Precision: 0.5387, Recall: 0.5386, F1 Score: 0.5374\n",
      "Epoch [9/20], Train Loss: 1.2178, Val Loss: 1.1353, Val Acc: 53.87%\n",
      "Validation - Precision: 0.5437, Recall: 0.5459, F1 Score: 0.5438\n",
      "Epoch [10/20], Train Loss: 1.1909, Val Loss: 1.1924, Val Acc: 54.61%\n",
      "New Best Model Saved! Validation Accuracy: 54.61%\n",
      "Validation - Precision: 0.5499, Recall: 0.5519, F1 Score: 0.5488\n",
      "Epoch [11/20], Train Loss: 1.1291, Val Loss: 1.2342, Val Acc: 55.14%\n",
      "New Best Model Saved! Validation Accuracy: 55.14%\n",
      "Validation - Precision: 0.5566, Recall: 0.5553, F1 Score: 0.5546\n",
      "Epoch [12/20], Train Loss: 1.1051, Val Loss: 1.0603, Val Acc: 55.57%\n",
      "New Best Model Saved! Validation Accuracy: 55.57%\n",
      "Validation - Precision: 0.5568, Recall: 0.5598, F1 Score: 0.5573\n",
      "Epoch [13/20], Train Loss: 1.0855, Val Loss: 1.1327, Val Acc: 55.99%\n",
      "New Best Model Saved! Validation Accuracy: 55.99%\n",
      "Validation - Precision: 0.5626, Recall: 0.5636, F1 Score: 0.5623\n",
      "Epoch [14/20], Train Loss: 1.0716, Val Loss: 1.1326, Val Acc: 56.38%\n",
      "New Best Model Saved! Validation Accuracy: 56.38%\n",
      "Validation - Precision: 0.5615, Recall: 0.5592, F1 Score: 0.5592\n",
      "Epoch [15/20], Train Loss: 1.0556, Val Loss: 1.0003, Val Acc: 55.94%\n",
      "Validation - Precision: 0.5666, Recall: 0.5642, F1 Score: 0.5640\n",
      "Epoch [16/20], Train Loss: 1.0188, Val Loss: 1.0605, Val Acc: 56.41%\n",
      "New Best Model Saved! Validation Accuracy: 56.41%\n",
      "Validation - Precision: 0.5674, Recall: 0.5667, F1 Score: 0.5659\n",
      "Epoch [17/20], Train Loss: 0.9970, Val Loss: 1.1220, Val Acc: 56.63%\n",
      "New Best Model Saved! Validation Accuracy: 56.63%\n",
      "Validation - Precision: 0.5655, Recall: 0.5641, F1 Score: 0.5640\n",
      "Epoch [18/20], Train Loss: 0.9889, Val Loss: 1.1030, Val Acc: 56.42%\n",
      "Validation - Precision: 0.5698, Recall: 0.5676, F1 Score: 0.5676\n",
      "Epoch [19/20], Train Loss: 0.9788, Val Loss: 1.0682, Val Acc: 56.74%\n",
      "New Best Model Saved! Validation Accuracy: 56.74%\n",
      "Validation - Precision: 0.5703, Recall: 0.5700, F1 Score: 0.5698\n",
      "Epoch [20/20], Train Loss: 0.9749, Val Loss: 1.0967, Val Acc: 57.02%\n",
      "New Best Model Saved! Validation Accuracy: 57.02%\n",
      "Test Accuracy: 57.40%\n",
      "Test Precision: 0.5735, Recall: 0.5740, F1 Score: 0.5734\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# GPU 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 학습 데이터 증강 설정 (후반 학습용)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # 반환:(최댓값, 인덱스)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlWGqvZNnC1E"
   },
   "source": [
    "###📌 **CUSTOM DATASET**\n",
    "\n",
    "1. `CustomDataset` 클래스 추가:\n",
    "- 지정된 경로에서 이미지 데이터를 폴더 구조로 로드\n",
    "- 폴더 이름이 클래스 이름으로 간주 (예: ./data/train/class1, ./data/train/class2 등)\n",
    "- 지원되는 이미지 확장자: .png, .jpg, .jpeg\n",
    "\n",
    " 📌 **사용 방법**\n",
    "- 학습 데이터 경로: `./data/train`\n",
    "- 테스트 데이터 경로: `./data/test`\n",
    "- **폴더 구조**는 다음과 같은 방식으로 준비해야 합니다:\n",
    "\n",
    "```bash\n",
    "./data/train/class1/ 이미지1.jpg, 이미지2.jpg, ...\n",
    "./data/train/class2/ 이미지1.jpg, 이미지2.jpg, ...\n",
    "./data/test/class1/ 이미지1.jpg, 이미지2.jpg, ...\n",
    "./data/test/class2/ 이미지1.jpg, 이미지2.jpg, ...\n",
    "```\n",
    "\n",
    "2. 보편적 정규화 값 적용:\n",
    "\n",
    "```python\n",
    "CUSTOM_MEAN = (0.5, 0.5, 0.5)\n",
    "CUSTOM_STD = (0.5, 0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K5t3H52nDTX"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋 사용 시 정규화 평균과 표준편차 설정\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "# 자체 데이터셋 사용 시 정규화 평균과 표준편차 설정\n",
    "CUSTOM_MEAN = (0.5, 0.5, 0.5)\n",
    "CUSTOM_STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "# 약한 데이터 증강 설정 (초기 학습용)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CUSTOM_MEAN, CUSTOM_STD)\n",
    "])\n",
    "\n",
    "# 강한 데이터 증강 설정 (후반 학습용)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CUSTOM_MEAN, CUSTOM_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CUSTOM_MEAN, CUSTOM_STD)\n",
    "])\n",
    "\n",
    "# 자체 데이터셋 로드 클래스 정의\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        classes = os.listdir(root_dir)  # 폴더 이름을 클래스로 사용\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root_dir, cls_name)\n",
    "            if os.path.isdir(cls_folder):\n",
    "                for img_name in os.listdir(cls_folder):\n",
    "                    img_path = os.path.join(cls_folder, img_name)\n",
    "                    if img_path.endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일만 추가\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "train_dir = './data/train'\n",
    "test_dir = './data/test'\n",
    "\n",
    "# 자체 데이터셋 로드\n",
    "full_train_dataset = CustomDataset(root_dir=train_dir, transform=weak_transform)\n",
    "test_dataset = CustomDataset(root_dir=test_dir, transform=transform_test)\n",
    "\n",
    "# full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "# test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의 (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP()\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # 에폭 절반 이후 강한 데이터 증강으로 전환\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()  # 모델 로드 시도\n",
    "    train()\n",
    "    test()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZMpivB4aIIuUZllY3NiWj",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
