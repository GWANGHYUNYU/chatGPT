{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytqiMA2q-lcG"
   },
   "source": [
    "## PYTORCH MLP ëª¨ë¸\n",
    "\n",
    "- CIFAR10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ MLP ëª¨ë¸ì„ í›ˆë ¨ ë° í‰ê°€í•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤\n",
    "- ëª¨ë¸ì€ Fully Connected Layerë“¤ë¡œ êµ¬ì„±\n",
    "- í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ReLUë¥¼ ì‚¬ìš©\n",
    "- í›ˆë ¨ í•¨ìˆ˜ì™€ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë„ í¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281623,
     "status": "ok",
     "timestamp": 1743000474589,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "O937L-sivzUw",
    "outputId": "1a97fd43-8084-448b-d931-b9b20c91634b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:10<00:00, 15.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6538\n",
      "Epoch [2/10], Loss: 1.4355\n",
      "Epoch [3/10], Loss: 1.3241\n",
      "Epoch [4/10], Loss: 1.2271\n",
      "Epoch [5/10], Loss: 1.1404\n",
      "Epoch [6/10], Loss: 1.0678\n",
      "Epoch [7/10], Loss: 0.9954\n",
      "Epoch [8/10], Loss: 0.9235\n",
      "Epoch [9/10], Loss: 0.8585\n",
      "Epoch [10/10], Loss: 0.7944\n",
      "Test Accuracy: 54.05%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsOnCmIF-WQ1"
   },
   "source": [
    "## ğŸ“Œ **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìˆ˜ì •**\n",
    "\n",
    "### Weight Decay (ê°€ì¤‘ì¹˜ ê°ì‡ )\n",
    "- `weight_decay=0.0001`ìœ¼ë¡œ **Adam ì˜µí‹°ë§ˆì´ì €**ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### Dropout Rate (ë“œë¡­ì•„ì›ƒ í™•ë¥ )\n",
    "- MLP ëª¨ë¸ì˜ ê° ë ˆì´ì–´ ì‚¬ì´ì— `nn.Dropout(0.5)`ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### Batch Normalization (ë°°ì¹˜ ì •ê·œí™”)\n",
    "- **ë ˆì´ì–´ë§ˆë‹¤** `nn.BatchNorm1d()`ë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### Learning Rate Scheduler (í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬)\n",
    "- `torch.optim.lr_scheduler.StepLR`ì„ ì´ìš©í•˜ì—¬ **ë§¤ ì—í­ 5íšŒë§ˆë‹¤ í•™ìŠµë¥ ì„ 10%ë¡œ ê°ì†Œ**ì‹œí‚¤ë„ë¡ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### Activation Function (í™œì„±í™” í•¨ìˆ˜)\n",
    "- ë§ˆì§€ë§‰ **Hidden Layer**ëŠ” `nn.LeakyReLU(0.1)`ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272512,
     "status": "ok",
     "timestamp": 1743000763460,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "ZMAV6Xqv7KbD",
    "outputId": "607087d9-bbf0-4bdd-e453-074f0ff3ce6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.7861\n",
      "Epoch [2/10], Loss: 1.6285\n",
      "Epoch [3/10], Loss: 1.5698\n",
      "Epoch [4/10], Loss: 1.5375\n",
      "Epoch [5/10], Loss: 1.5103\n",
      "Epoch [6/10], Loss: 1.4344\n",
      "Epoch [7/10], Loss: 1.3983\n",
      "Epoch [8/10], Loss: 1.3852\n",
      "Epoch [9/10], Loss: 1.3641\n",
      "Epoch [10/10], Loss: 1.3533\n",
      "Test Accuracy: 53.97%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001  # ê°€ì¤‘ì¹˜ ê°ì‡ \n",
    "DROPOUT_RATE = 0.5  # ë“œë¡­ì•„ì›ƒ í™•ë¥ \n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # í•™ìŠµë¥  ê°ì†Œ\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Op6BHk9cm5"
   },
   "source": [
    "### ğŸ“Œ ë ˆì´ì–´ ì¶”ê°€ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½\n",
    "\n",
    "### ğŸ”¨ ë ˆì´ì–´ ì¶”ê°€\n",
    "- ê¸°ì¡´ì˜ ëª¨ë¸ êµ¬ì¡°ì— **`nn.Linear(256, 256)`** ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.  \n",
    "- ì¶”ê°€ëœ ë ˆì´ì–´ ì´í›„ì—ë„ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ **Batch Normalization, ReLU, Dropout**ì„ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "nn.BatchNorm1d(256),\n",
    "nn.ReLU(),\n",
    "nn.Dropout(DROPOUT_RATE),\n",
    "```\n",
    "\n",
    "### ğŸ”¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½\n",
    "- **DROPOUT_RATE**: 0.5 â” 0.3 (ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¡°ì •)\n",
    "- **WEIGHT_DECAY**: 0.0001 â” 1e-5 (ëª¨ë¸ì˜ í•™ìŠµ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´ ë‚®ì¶¤)\n",
    "\n",
    "### ğŸ”¨ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "- í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨ì„ 0.1 â” 0.5 ë¡œ ë³€ê²½í•˜ì—¬ ë„ˆë¬´ ê¸‰ê²©íˆ ê°ì†Œí•˜ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308141,
     "status": "ok",
     "timestamp": 1743001381660,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "syeTs__58k5A",
    "outputId": "4f801bfb-d86a-4d42-8b95-1935e2c0a472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.7386\n",
      "Best model saved with loss: 1.7386\n",
      "Epoch [2/10], Loss: 1.5618\n",
      "Best model saved with loss: 1.5618\n",
      "Epoch [3/10], Loss: 1.4865\n",
      "Best model saved with loss: 1.4865\n",
      "Epoch [4/10], Loss: 1.4331\n",
      "Best model saved with loss: 1.4331\n",
      "Epoch [5/10], Loss: 1.3920\n",
      "Best model saved with loss: 1.3920\n",
      "Epoch [6/10], Loss: 1.3116\n",
      "Best model saved with loss: 1.3116\n",
      "Epoch [7/10], Loss: 1.2709\n",
      "Best model saved with loss: 1.2709\n",
      "Epoch [8/10], Loss: 1.2503\n",
      "Best model saved with loss: 1.2503\n",
      "Epoch [9/10], Loss: 1.2269\n",
      "Best model saved with loss: 1.2269\n",
      "Epoch [10/10], Loss: 1.1999\n",
      "Best model saved with loss: 1.1999\n",
      "Test Accuracy: 56.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì¤„ì„\n",
    "DROPOUT_RATE = 0.3  # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ë‚®ì¶¤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "MODEL_PATH = './best_model.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),  # LeakyReLU ëŒ€ì‹  ReLU ì‚¬ìš©\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "\n",
    "best_loss = float('inf')  # ì´ˆê¸°ê°’ì„ ë¬´í•œëŒ€ë¡œ ì„¤ì •\n",
    "\n",
    "def train():\n",
    "    global best_loss\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # í•™ìŠµë¥  ê°ì†Œ\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥: ë² ìŠ¤íŠ¸ ë¡œìŠ¤ì¼ ê²½ìš°\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp12G_-m_Xoh"
   },
   "source": [
    "### ğŸ“Œ **ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©**\n",
    "- `RandomHorizontalFlip()`: ì´ë¯¸ì§€ë¥¼ ì¢Œìš°ë¡œ ë¬´ì‘ìœ„ë¡œ ë’¤ì§‘ìŒ.\n",
    "- `RandomCrop(32, padding=4)`: ì›ë˜ í¬ê¸°ë¡œ ë˜ëŒë¦¬ê¸° ì „ì— íŒ¨ë”©ì„ ì¶”ê°€í•˜ì—¬ ì„ì˜ì˜ ë¶€ë¶„ì„ ì˜ë¼ëƒ„.\n",
    "- `ColorJitter()`: ë°ê¸°, ëŒ€ë¹„, ì±„ë„, ìƒ‰ì¡°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë³€ê²½.\n",
    "- `RandomRotation(10)`: ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ 10ë„ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ë¡œ íšŒì „.\n",
    "\n",
    "```python\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "```\n",
    "\n",
    "- ë°ì´í„° ì¦ê°•ì€ ë°ì´í„°ì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í•™ìŠµ ê³¼ì •ì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Œ **ë°ì´í„° ì¦ê°•ì˜ íŠ¹ì§•**\n",
    "- ë°ì´í„° ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤. (`train_dataset`ì˜ í¬ê¸°ëŠ” ë™ì¼í•©ë‹ˆë‹¤.)\n",
    "- ë§¤ë²ˆ ë¯¸ë‹ˆë°°ì¹˜ê°€ ëª¨ë¸ì— ì „ë‹¬ë  ë•Œ, ë™ì ìœ¼ë¡œ ë³€í˜•ì´ ì ìš©ë©ë‹ˆë‹¤.\n",
    "- ê°™ì€ ì´ë¯¸ì§€ë¼ í•˜ë”ë¼ë„ ë§¤ë²ˆ ë‹¤ë¥¸ í˜•íƒœ(íšŒì „, ìƒ‰ìƒ ë³€ê²½, ì¢Œìš° ë°˜ì „ ë“±)ë¡œ ëª¨ë¸ì— ì…ë ¥ë©ë‹ˆë‹¤.\n",
    "- ì¦ê°•ëœ ì´ë¯¸ì§€ëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì§€ ì•Šê³ , ë§¤ë²ˆ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Œ **ì˜ˆì‹œ**\n",
    "- í•™ìŠµ ë°ì´í„°ì…‹ì˜ í¬ê¸°ëŠ” ì—¬ì „íˆ `len(train_dataset) = 50,000` ì…ë‹ˆë‹¤.\n",
    "- í•˜ì§€ë§Œ ê° ì—í¬í¬ë§ˆë‹¤ ëª¨ë¸ì€ ì™„ì „íˆ ë‹¤ë¥¸ í˜•íƒœì˜ ë³€í˜•ëœ ì´ë¯¸ì§€ë¥¼ ë³´ê²Œ ë©ë‹ˆë‹¤.\n",
    "- ê²°ê³¼ì ìœ¼ë¡œëŠ” ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ëŠ˜ë ¤ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 692621,
     "status": "ok",
     "timestamp": 1743002300480,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "7FtPwtXT_ch8",
    "outputId": "4a32743c-bb37-444c-a2fe-f7730cf732f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.9366\n",
      "Best model saved with loss: 1.9366\n",
      "Epoch [2/10], Loss: 1.8013\n",
      "Best model saved with loss: 1.8013\n",
      "Epoch [3/10], Loss: 1.7497\n",
      "Best model saved with loss: 1.7497\n",
      "Epoch [4/10], Loss: 1.7116\n",
      "Best model saved with loss: 1.7116\n",
      "Epoch [5/10], Loss: 1.6841\n",
      "Best model saved with loss: 1.6841\n",
      "Epoch [6/10], Loss: 1.6448\n",
      "Best model saved with loss: 1.6448\n",
      "Epoch [7/10], Loss: 1.6258\n",
      "Best model saved with loss: 1.6258\n",
      "Epoch [8/10], Loss: 1.6110\n",
      "Best model saved with loss: 1.6110\n",
      "Epoch [9/10], Loss: 1.6040\n",
      "Best model saved with loss: 1.6040\n",
      "Epoch [10/10], Loss: 1.5969\n",
      "Best model saved with loss: 1.5969\n",
      "Test Accuracy: 50.23%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì¤„ì„\n",
    "DROPOUT_RATE = 0.3  # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ë‚®ì¶¤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "MODEL_PATH = './best_model.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜ (ë°ì´í„° ì¦ê°• ì¶”ê°€)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "\n",
    "best_loss = float('inf')  # ì´ˆê¸°ê°’ì„ ë¬´í•œëŒ€ë¡œ ì„¤ì •\n",
    "\n",
    "def train():\n",
    "    global best_loss\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # í•™ìŠµë¥  ê°ì†Œ\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥: ë² ìŠ¤íŠ¸ ë¡œìŠ¤ì¼ ê²½ìš°\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjhq5d-PDZ26"
   },
   "source": [
    "### ğŸ“Œ **ë°ì´í„° ì¦ê°• ê°•ë„ ë³€ê²½**\n",
    "\n",
    "1. ë°ì´í„° ì¦ê°• ê°•ë„ ê°ì†Œ:\n",
    "- ColorJitter: brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05 ë¡œ ì¤„ì„.\n",
    "- RandomRotation: 10 â” 5 ë¡œ ì¤„ì„.\n",
    "\n",
    "2. í•™ìŠµ ì—í­ ì¦ê°€:\n",
    "- EPOCHS = 10 â” 20 ìœ¼ë¡œ ì¦ê°€.\n",
    "\n",
    "3. ë‘ ê°€ì§€ ë°ì´í„° ì¦ê°• ê¸°ë²• ì •ì˜:\n",
    "- weak_transform: ì´ˆê¸° í•™ìŠµì— ì‚¬ìš©, ê°€ë²¼ìš´ ë°ì´í„° ì¦ê°• (RandomHorizontalFlip, RandomCrop ë§Œ ì ìš©)\n",
    "- strong_transform: í›„ë°˜ í•™ìŠµì— ì‚¬ìš©, ê°•í•œ ë°ì´í„° ì¦ê°• (ColorJitter, RandomRotation ì¶”ê°€)\n",
    "\n",
    "4. ì—í­ì˜ ì ˆë°˜ ì´í›„ ë°ì´í„° ì¦ê°• ê¸°ë²• êµì²´:\n",
    "\n",
    "```python\n",
    "if epoch == EPOCHS // 2:\n",
    "    train_dataset.transform = strong_transform\n",
    "    print(\"Switched to Strong Data Augmentation\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GhlQcaOD9RB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # ì—í­ ìˆ˜ ì¦ê°€\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì¤„ì„\n",
    "DROPOUT_RATE = 0.3  # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ë‚®ì¶¤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "MODEL_PATH = './best_model.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "# CIFAR-10 ë°ì´í„°ì…‹ì˜ ì‹¤ì œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì‚¬ìš©\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ (ì´ˆê¸°ì—ëŠ” ì•½í•œ ë°ì´í„° ì¦ê°• ì‚¬ìš©)\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "\n",
    "best_loss = float('inf')  # ì´ˆê¸°ê°’ì„ ë¬´í•œëŒ€ë¡œ ì„¤ì •\n",
    "\n",
    "def train():\n",
    "    global best_loss, train_dataset\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ êµì²´\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.transform = strong_transform\n",
    "            print(\"Switched to Strong Data Augmentation\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # í•™ìŠµë¥  ê°ì†Œ\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥: ë² ìŠ¤íŠ¸ ë¡œìŠ¤ì¼ ê²½ìš°\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ubl4ZTcAusv"
   },
   "source": [
    "### ğŸ“Œ **CIFAR10 ì •ê·œí™”ê°’ ë³€ê²½**\n",
    "\n",
    "- CIFAR-10ì˜ í‰ê· : (0.4914, 0.4822, 0.4465)\n",
    "\n",
    "- CIFAR-10ì˜ í‘œì¤€í¸ì°¨: (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "```python\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 678573,
     "status": "ok",
     "timestamp": 1743003000562,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "9L2t74lpByvJ",
    "outputId": "fde3f18e-5212-4199-fff5-d973626498c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 1.9369\n",
      "Best model saved with loss: 1.9369\n",
      "Epoch [2/10], Loss: 1.8046\n",
      "Best model saved with loss: 1.8046\n",
      "Epoch [3/10], Loss: 1.7432\n",
      "Best model saved with loss: 1.7432\n",
      "Epoch [4/10], Loss: 1.7115\n",
      "Best model saved with loss: 1.7115\n",
      "Epoch [5/10], Loss: 1.6846\n",
      "Best model saved with loss: 1.6846\n",
      "Epoch [6/10], Loss: 1.6456\n",
      "Best model saved with loss: 1.6456\n",
      "Epoch [7/10], Loss: 1.6194\n",
      "Best model saved with loss: 1.6194\n",
      "Epoch [8/10], Loss: 1.6148\n",
      "Best model saved with loss: 1.6148\n",
      "Epoch [9/10], Loss: 1.6003\n",
      "Best model saved with loss: 1.6003\n",
      "Epoch [10/10], Loss: 1.5886\n",
      "Best model saved with loss: 1.5886\n",
      "Test Accuracy: 49.81%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì¤„ì„\n",
    "DROPOUT_RATE = 0.3  # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ë‚®ì¶¤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "MODEL_PATH = './best_model.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "# CIFAR-10 ë°ì´í„°ì…‹ì˜ ì‹¤ì œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì‚¬ìš©\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜ (ë°ì´í„° ì¦ê°• ì¶”ê°€)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "\n",
    "best_loss = float('inf')  # ì´ˆê¸°ê°’ì„ ë¬´í•œëŒ€ë¡œ ì„¤ì •\n",
    "\n",
    "def train():\n",
    "    global best_loss\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # í•™ìŠµë¥  ê°ì†Œ\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥: ë² ìŠ¤íŠ¸ ë¡œìŠ¤ì¼ ê²½ìš°\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vILZbWQ0E4LZ"
   },
   "source": [
    "###ğŸ“Œ **ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥ ì¶”ê°€**\n",
    "\n",
    "1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜ (load_model) ì¶”ê°€:\n",
    "\n",
    "```python\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "```\n",
    "- í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ì €ì¥ëœ ëª¨ë¸(`best_model.pth`)ì´ ì¡´ì¬í•˜ë©´ ì´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "- ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì½”ë“œ ì¶”ê°€ (`main()` í•¨ìˆ˜ ë‚´):\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()  # ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 802729,
     "status": "ok",
     "timestamp": 1743077016996,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "4kItpKG1FMZZ",
    "outputId": "43a7b8b6-dee3-48fe-9afa-58ba3241217a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:01<00:00, 86.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n",
      "Epoch [1/20], Loss: 1.8899\n",
      "Best model saved with loss: 1.8899\n",
      "Epoch [2/20], Loss: 1.7429\n",
      "Best model saved with loss: 1.7429\n",
      "Epoch [3/20], Loss: 1.6875\n",
      "Best model saved with loss: 1.6875\n",
      "Epoch [4/20], Loss: 1.6513\n",
      "Best model saved with loss: 1.6513\n",
      "Epoch [5/20], Loss: 1.6220\n",
      "Best model saved with loss: 1.6220\n",
      "Epoch [6/20], Loss: 1.5676\n",
      "Best model saved with loss: 1.5676\n",
      "Epoch [7/20], Loss: 1.5471\n",
      "Best model saved with loss: 1.5471\n",
      "Epoch [8/20], Loss: 1.5373\n",
      "Best model saved with loss: 1.5373\n",
      "Epoch [9/20], Loss: 1.5266\n",
      "Best model saved with loss: 1.5266\n",
      "Epoch [10/20], Loss: 1.5135\n",
      "Best model saved with loss: 1.5135\n",
      "Switched to Strong Data Augmentation\n",
      "Epoch [11/20], Loss: 1.5187\n",
      "Epoch [12/20], Loss: 1.5073\n",
      "Best model saved with loss: 1.5073\n",
      "Epoch [13/20], Loss: 1.5022\n",
      "Best model saved with loss: 1.5022\n",
      "Epoch [14/20], Loss: 1.4918\n",
      "Best model saved with loss: 1.4918\n",
      "Epoch [15/20], Loss: 1.4858\n",
      "Best model saved with loss: 1.4858\n",
      "Epoch [16/20], Loss: 1.4766\n",
      "Best model saved with loss: 1.4766\n",
      "Epoch [17/20], Loss: 1.4661\n",
      "Best model saved with loss: 1.4661\n",
      "Epoch [18/20], Loss: 1.4628\n",
      "Best model saved with loss: 1.4628\n",
      "Epoch [19/20], Loss: 1.4653\n",
      "Epoch [20/20], Loss: 1.4563\n",
      "Best model saved with loss: 1.4563\n",
      "Test Accuracy: 53.96%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # ì—í­ ìˆ˜ ì¦ê°€\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì¤„ì„\n",
    "DROPOUT_RATE = 0.3  # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ë‚®ì¶¤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "MODEL_PATH = './best_model.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "# CIFAR-10 ë°ì´í„°ì…‹ì˜ ì‹¤ì œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì‚¬ìš©\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ (ì´ˆê¸°ì—ëŠ” ì•½í•œ ë°ì´í„° ì¦ê°• ì‚¬ìš©)\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "\n",
    "best_loss = float('inf')  # ì´ˆê¸°ê°’ì„ ë¬´í•œëŒ€ë¡œ ì„¤ì •\n",
    "\n",
    "def train():\n",
    "    global best_loss, train_dataset\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ êµì²´\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.transform = strong_transform\n",
    "            print(\"Switched to Strong Data Augmentation\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()  # í•™ìŠµë¥  ê°ì†Œ\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥: ë² ìŠ¤íŠ¸ ë¡œìŠ¤ì¼ ê²½ìš°\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()  # ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi0MuTNNGH_2"
   },
   "source": [
    "###ğŸ“Œ **ëª¨ë¸ ì‹œê°í™”**\n",
    "\n",
    "1. torchviz ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©:\n",
    "- ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ì—¬ MLP_Model_Structure.png ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. ëª¨ë¸ ì‹œê°í™” í•¨ìˆ˜ (visualize_model) ì¶”ê°€:\n",
    "\n",
    "```python\n",
    "def visualize_model():\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)  # CIFAR10 ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ\n",
    "    output = model(dummy_input)\n",
    "    graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "    graph.render(\"MLP_Model_Structure\", format=\"png\", cleanup=True)\n",
    "    print(\"Model visualization saved as 'MLP_Model_Structure.png'\")\n",
    "```\n",
    "\n",
    "3. ëª¨ë¸ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ:\n",
    "\n",
    "```python\n",
    "visualize_model()  # ëª¨ë¸ ì‹œê°í™” í˜¸ì¶œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FbNCtAGJdm"
   },
   "outputs": [],
   "source": [
    "# !pip install torchviz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torchviz import make_dot  # ëª¨ë¸ ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # ì—í­ ìˆ˜ ì¦ê°€\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì¤„ì„\n",
    "DROPOUT_RATE = 0.3  # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ë‚®ì¶¤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 ì´ë¯¸ì§€ì˜ Flatten í¬ê¸°\n",
    "NUM_CLASSES = 10  # CIFAR10ì€ 10ê°œì˜ í´ë˜ìŠ¤\n",
    "MODEL_PATH = './best_model.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "# CIFAR-10 ë°ì´í„°ì…‹ì˜ ì‹¤ì œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì‚¬ìš©\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ (ì´ˆê¸°ì—ëŠ” ì•½í•œ ë°ì´í„° ì¦ê°• ì‚¬ìš©)\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP ëª¨ë¸ ì •ì˜\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),  # ì¶”ê°€ëœ ë ˆì´ì–´\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# ëª¨ë¸ ì‹œê°í™” í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def visualize_model():\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)  # CIFAR10 ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ\n",
    "    output = model(dummy_input)\n",
    "    graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "    graph.render(\"MLP_Model_Structure\", format=\"png\", cleanup=True)\n",
    "    print(\"Model visualization saved as 'MLP_Model_Structure.png'\")\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½\n",
    "\n",
    "best_loss = float('inf')  # ì´ˆê¸°ê°’ì„ ë¬´í•œëŒ€ë¡œ ì„¤ì •\n",
    "\n",
    "def train():\n",
    "    global best_loss, train_dataset\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.transform = strong_transform\n",
    "            print(\"Switched to Strong Data Augmentation\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()\n",
    "    visualize_model()  # ëª¨ë¸ ì‹œê°í™” í˜¸ì¶œ\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q0EcBrQL4kD"
   },
   "source": [
    "###ğŸ“Œ **Train/Validation/Test ë°ì´í„°ì…‹ í™œ**\n",
    "\n",
    "1. í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ê¸° (Train-Validation Split)\n",
    "- ë°ì´í„°ì…‹ ë¶„í• ì„ ìœ„í•œ `random_split()` ì‚¬ìš©\n",
    "- í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— `SPLIT_RATIO`ë¥¼ ì„¤ì •í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ ë¹„ìœ¨ ì„¤ì •\n",
    "\n",
    "2. í•™ìŠµ ì½”ë“œ ìˆ˜ì •í•˜ê¸° (Train & Validation)\n",
    "- ê¸°ì¡´ì˜ `train()` í•¨ìˆ˜ì—ì„œ í•™ìŠµ í›„ ë§¤ ì—í­ë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ë¥¼ í‰ê°€í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€\n",
    "\n",
    "\n",
    "3. ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€í•˜ê¸°\n",
    "- `validate()` í•¨ìˆ˜\n",
    "\n",
    "```python\n",
    "def validate():\n",
    "    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "    return avg_loss, accuracy\n",
    "```\n",
    "\n",
    "4. Metric ì¶”ê°€\n",
    "- **Precision**, **Recall**, **F1-Score** ì¶”ê°€ (ë§¤ ì—í­ë§ˆë‹¤ ê²€ì¦ ì‹œ ê³„ì‚°)\n",
    "\n",
    "5. Early Stopping ìˆ˜ì •\n",
    "- Validation Accuracy(`best_val_acc`)ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´, `patience`ê°€ 3ì¼ ë•Œ ì¡°ê¸° ì¢…ë£Œ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334606,
     "status": "ok",
     "timestamp": 1743079163248,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "OjAIovzcjeOX",
    "outputId": "fbdfd395-47e6-4451-e5a5-ec3c2b9b894b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4376, Recall: 0.4379, F1 Score: 0.4288\n",
      "Epoch [1/20], Train Loss: 1.7689, Val Loss: 1.2486, Val Acc: 43.73%\n",
      "New Best Model Saved! Validation Accuracy: 43.73%\n",
      "Validation - Precision: 0.4723, Recall: 0.4722, F1 Score: 0.4660\n",
      "Epoch [2/20], Train Loss: 1.5828, Val Loss: 1.1320, Val Acc: 47.11%\n",
      "New Best Model Saved! Validation Accuracy: 47.11%\n",
      "Validation - Precision: 0.4895, Recall: 0.4881, F1 Score: 0.4818\n",
      "Epoch [3/20], Train Loss: 1.5040, Val Loss: 1.1204, Val Acc: 48.70%\n",
      "New Best Model Saved! Validation Accuracy: 48.70%\n",
      "Validation - Precision: 0.4985, Recall: 0.4995, F1 Score: 0.4946\n",
      "Epoch [4/20], Train Loss: 1.4524, Val Loss: 1.1050, Val Acc: 49.96%\n",
      "New Best Model Saved! Validation Accuracy: 49.96%\n",
      "Validation - Precision: 0.5107, Recall: 0.5117, F1 Score: 0.5066\n",
      "Epoch [5/20], Train Loss: 1.4065, Val Loss: 1.1460, Val Acc: 51.13%\n",
      "New Best Model Saved! Validation Accuracy: 51.13%\n",
      "Validation - Precision: 0.5234, Recall: 0.5263, F1 Score: 0.5211\n",
      "Epoch [6/20], Train Loss: 1.3189, Val Loss: 1.0692, Val Acc: 52.57%\n",
      "New Best Model Saved! Validation Accuracy: 52.57%\n",
      "Validation - Precision: 0.5257, Recall: 0.5292, F1 Score: 0.5260\n",
      "Epoch [7/20], Train Loss: 1.2895, Val Loss: 0.8917, Val Acc: 52.91%\n",
      "New Best Model Saved! Validation Accuracy: 52.91%\n",
      "Validation - Precision: 0.5398, Recall: 0.5383, F1 Score: 0.5360\n",
      "Epoch [8/20], Train Loss: 1.2538, Val Loss: 1.0956, Val Acc: 53.78%\n",
      "New Best Model Saved! Validation Accuracy: 53.78%\n",
      "Validation - Precision: 0.5383, Recall: 0.5394, F1 Score: 0.5355\n",
      "Epoch [9/20], Train Loss: 1.2374, Val Loss: 1.0138, Val Acc: 53.98%\n",
      "New Best Model Saved! Validation Accuracy: 53.98%\n",
      "Validation - Precision: 0.5436, Recall: 0.5476, F1 Score: 0.5444\n",
      "Epoch [10/20], Train Loss: 1.2088, Val Loss: 1.0396, Val Acc: 54.74%\n",
      "New Best Model Saved! Validation Accuracy: 54.74%\n",
      "Validation - Precision: 0.4475, Recall: 0.4566, F1 Score: 0.4477\n",
      "Epoch [11/20], Train Loss: 1.6474, Val Loss: 1.2091, Val Acc: 45.62%\n",
      "Validation - Precision: 0.4612, Recall: 0.4659, F1 Score: 0.4601\n",
      "Epoch [12/20], Train Loss: 1.5805, Val Loss: 1.2213, Val Acc: 46.52%\n",
      "Validation - Precision: 0.4749, Recall: 0.4763, F1 Score: 0.4718\n",
      "Epoch [13/20], Train Loss: 1.5554, Val Loss: 1.2802, Val Acc: 47.62%\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 55.46%\n",
      "Test Precision: 0.5502, Recall: 0.5546, F1 Score: 0.5511\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 ë°ì´í„°ì…‹ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ (ì •ê·œí™” ê¸°ì¤€)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ì ìš©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì „í™˜\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping ì²´í¬\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()  # ëª¨ë¸ ë¡œë“œ ì‹œë„\n",
    "    train()\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXNRlD6fpeDB"
   },
   "source": [
    "###ğŸ“Œ **í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„œì¹˜**\n",
    "\n",
    "1. **Optuna** ì‚¬ìš©\n",
    "- `import optuna`\n",
    "\n",
    "2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ëŒ€ìƒ:\n",
    "\n",
    "- `batch_size` (32, 64, 128)\n",
    "- `learning_rate` (1e-4 ~ 1e-2)\n",
    "- `dropout_rate` (0.1 ~ 0.5)\n",
    "- `optimizer` (Adam, SGD)\n",
    "\n",
    "3. í•™ìŠµ ì¤‘ê°„ ê²€ì¦:\n",
    "- ë§¤ ì—í­ë§ˆë‹¤ `trial.report()` ë¡œ ê²°ê³¼ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤.\n",
    "- ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ `Optuna`ê°€ í•™ìŠµì„ ì¡°ê¸° ì¤‘ë‹¨ (`TrialPruned()`)í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. í•™ìŠµ ì™„ë£Œ í›„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCrdKDF_qYAT",
    "outputId": "75c9064f-6c10-4bc0-e608-d6f33e5fc20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:06:13,870] A new study created in memory with name: no-name-1dab4e45-5ddc-423f-8608-eb1c86f15e22\n",
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4291, Recall: 0.4247, F1 Score: 0.4202\n",
      "Validation - Precision: 0.4652, Recall: 0.4653, F1 Score: 0.4588\n",
      "Validation - Precision: 0.4857, Recall: 0.4858, F1 Score: 0.4804\n",
      "Validation - Precision: 0.5000, Recall: 0.5026, F1 Score: 0.4986\n",
      "Validation - Precision: 0.5125, Recall: 0.5137, F1 Score: 0.5079\n",
      "Validation - Precision: 0.5212, Recall: 0.5170, F1 Score: 0.5121\n",
      "Validation - Precision: 0.5296, Recall: 0.5266, F1 Score: 0.5233\n",
      "Validation - Precision: 0.5332, Recall: 0.5337, F1 Score: 0.5314\n",
      "Validation - Precision: 0.5402, Recall: 0.5411, F1 Score: 0.5374\n",
      "Validation - Precision: 0.5507, Recall: 0.5502, F1 Score: 0.5489\n",
      "Validation - Precision: 0.4498, Recall: 0.4479, F1 Score: 0.4442\n",
      "Validation - Precision: 0.4583, Recall: 0.4601, F1 Score: 0.4561\n",
      "Validation - Precision: 0.4635, Recall: 0.4647, F1 Score: 0.4579\n",
      "Validation - Precision: 0.4687, Recall: 0.4703, F1 Score: 0.4680\n",
      "Validation - Precision: 0.4721, Recall: 0.4755, F1 Score: 0.4699\n",
      "Validation - Precision: 0.4848, Recall: 0.4843, F1 Score: 0.4813\n",
      "Validation - Precision: 0.4763, Recall: 0.4802, F1 Score: 0.4761\n",
      "Validation - Precision: 0.4834, Recall: 0.4852, F1 Score: 0.4802\n",
      "Validation - Precision: 0.4840, Recall: 0.4868, F1 Score: 0.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:16:47,473] Trial 0 finished with value: 55.21 and parameters: {'batch_size': 128, 'learning_rate': 0.003109548697581316, 'dropout_rate': 0.22553735533002361, 'weight_decay': 4.508346108254859e-05, 'step_size': 8, 'gamma': 0.7972745415846182, 'optimizer': 'SGD'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4942, Recall: 0.4965, F1 Score: 0.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3357, Recall: 0.3373, F1 Score: 0.3296\n",
      "Validation - Precision: 0.3541, Recall: 0.3517, F1 Score: 0.3443\n",
      "Validation - Precision: 0.3536, Recall: 0.3549, F1 Score: 0.3410\n",
      "Validation - Precision: 0.3732, Recall: 0.3754, F1 Score: 0.3716\n",
      "Validation - Precision: 0.3756, Recall: 0.3697, F1 Score: 0.3517\n",
      "Validation - Precision: 0.3895, Recall: 0.3808, F1 Score: 0.3714\n",
      "Validation - Precision: 0.3818, Recall: 0.3759, F1 Score: 0.3666\n",
      "Validation - Precision: 0.4002, Recall: 0.3938, F1 Score: 0.3762\n",
      "Validation - Precision: 0.3987, Recall: 0.4015, F1 Score: 0.3905\n",
      "Validation - Precision: 0.4091, Recall: 0.4067, F1 Score: 0.3962\n",
      "Validation - Precision: 0.4096, Recall: 0.4087, F1 Score: 0.4062\n",
      "Validation - Precision: 0.3924, Recall: 0.3981, F1 Score: 0.3859\n",
      "Validation - Precision: 0.4088, Recall: 0.4097, F1 Score: 0.4038\n",
      "Validation - Precision: 0.4191, Recall: 0.4238, F1 Score: 0.4139\n",
      "Validation - Precision: 0.4349, Recall: 0.4380, F1 Score: 0.4301\n",
      "Validation - Precision: 0.4375, Recall: 0.4352, F1 Score: 0.4338\n",
      "Validation - Precision: 0.4375, Recall: 0.4387, F1 Score: 0.4345\n",
      "Validation - Precision: 0.4482, Recall: 0.4443, F1 Score: 0.4370\n",
      "Validation - Precision: 0.4346, Recall: 0.4357, F1 Score: 0.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:44:19,385] Trial 1 finished with value: 44.69 and parameters: {'batch_size': 64, 'learning_rate': 0.009320346498472817, 'dropout_rate': 0.237750765520911, 'weight_decay': 2.028978770068843e-05, 'step_size': 7, 'gamma': 0.6376061337271416, 'optimizer': 'Adam'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4314, Recall: 0.4368, F1 Score: 0.4292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3170, Recall: 0.3114, F1 Score: 0.3015\n",
      "Validation - Precision: 0.3439, Recall: 0.3416, F1 Score: 0.3344\n",
      "Validation - Precision: 0.3631, Recall: 0.3622, F1 Score: 0.3533\n",
      "Validation - Precision: 0.3802, Recall: 0.3757, F1 Score: 0.3709\n",
      "Validation - Precision: 0.3875, Recall: 0.3836, F1 Score: 0.3785\n",
      "Validation - Precision: 0.3950, Recall: 0.3926, F1 Score: 0.3880\n",
      "Validation - Precision: 0.4021, Recall: 0.3992, F1 Score: 0.3923\n",
      "Validation - Precision: 0.4083, Recall: 0.4060, F1 Score: 0.3997\n",
      "Validation - Precision: 0.4039, Recall: 0.4036, F1 Score: 0.3977\n",
      "Validation - Precision: 0.4176, Recall: 0.4152, F1 Score: 0.4082\n",
      "Validation - Precision: 0.4155, Recall: 0.4152, F1 Score: 0.4100\n",
      "Validation - Precision: 0.4122, Recall: 0.4117, F1 Score: 0.4053\n",
      "Validation - Precision: 0.4111, Recall: 0.4137, F1 Score: 0.4070\n",
      "Validation - Precision: 0.4256, Recall: 0.4245, F1 Score: 0.4186\n",
      "Validation - Precision: 0.4210, Recall: 0.4188, F1 Score: 0.4129\n",
      "Validation - Precision: 0.4181, Recall: 0.4188, F1 Score: 0.4122\n",
      "Validation - Precision: 0.4198, Recall: 0.4199, F1 Score: 0.4146\n",
      "Validation - Precision: 0.4219, Recall: 0.4231, F1 Score: 0.4175\n",
      "Validation - Precision: 0.4221, Recall: 0.4241, F1 Score: 0.4180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 14:01:31,641] Trial 2 finished with value: 42.63 and parameters: {'batch_size': 64, 'learning_rate': 0.0003805663988899745, 'dropout_rate': 0.2219431290557982, 'weight_decay': 1.3523658748252338e-06, 'step_size': 4, 'gamma': 0.507303455610086, 'optimizer': 'SGD'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4159, Recall: 0.4141, F1 Score: 0.4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3738, Recall: 0.3595, F1 Score: 0.3487\n",
      "Validation - Precision: 0.3874, Recall: 0.3855, F1 Score: 0.3757\n",
      "Validation - Precision: 0.4074, Recall: 0.4017, F1 Score: 0.3967\n",
      "Validation - Precision: 0.3946, Recall: 0.3958, F1 Score: 0.3834\n",
      "Validation - Precision: 0.4224, Recall: 0.4182, F1 Score: 0.4150\n",
      "Validation - Precision: 0.4203, Recall: 0.4176, F1 Score: 0.4092\n",
      "Validation - Precision: 0.4107, Recall: 0.4126, F1 Score: 0.4025\n",
      "Validation - Precision: 0.4166, Recall: 0.4203, F1 Score: 0.4137\n",
      "Validation - Precision: 0.4206, Recall: 0.4258, F1 Score: 0.4174\n",
      "Validation - Precision: 0.4574, Recall: 0.4597, F1 Score: 0.4551\n",
      "Validation - Precision: 0.4556, Recall: 0.4621, F1 Score: 0.4565\n",
      "Validation - Precision: 0.4621, Recall: 0.4650, F1 Score: 0.4610\n",
      "Validation - Precision: 0.4657, Recall: 0.4708, F1 Score: 0.4654\n",
      "Validation - Precision: 0.4629, Recall: 0.4665, F1 Score: 0.4633\n",
      "Validation - Precision: 0.4643, Recall: 0.4703, F1 Score: 0.4641\n",
      "Validation - Precision: 0.4671, Recall: 0.4744, F1 Score: 0.4668\n",
      "Validation - Precision: 0.4742, Recall: 0.4768, F1 Score: 0.4722\n",
      "Validation - Precision: 0.4746, Recall: 0.4791, F1 Score: 0.4748\n",
      "Validation - Precision: 0.4854, Recall: 0.4905, F1 Score: 0.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 14:24:14,852] Trial 3 finished with value: 50.38 and parameters: {'batch_size': 64, 'learning_rate': 0.0012269573803510066, 'dropout_rate': 0.1557565882173503, 'weight_decay': 0.0004650420504685621, 'step_size': 9, 'gamma': 0.35019103938049145, 'optimizer': 'Adam'}. Best is trial 0 with value: 55.21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4983, Recall: 0.5020, F1 Score: 0.4984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e46d4c73be8f>:102: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "<ipython-input-6-e46d4c73be8f>:103: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "<ipython-input-6-e46d4c73be8f>:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "<ipython-input-6-e46d4c73be8f>:106: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.3184, Recall: 0.2817, F1 Score: 0.2509\n",
      "Validation - Precision: 0.3304, Recall: 0.3233, F1 Score: 0.3040\n",
      "Validation - Precision: 0.3445, Recall: 0.3352, F1 Score: 0.3180\n",
      "Validation - Precision: 0.3631, Recall: 0.3532, F1 Score: 0.3407\n",
      "Validation - Precision: 0.3672, Recall: 0.3584, F1 Score: 0.3473\n",
      "Validation - Precision: 0.3848, Recall: 0.3793, F1 Score: 0.3675\n",
      "Validation - Precision: 0.3773, Recall: 0.3785, F1 Score: 0.3618\n",
      "Validation - Precision: 0.3929, Recall: 0.3902, F1 Score: 0.3785\n",
      "Validation - Precision: 0.3988, Recall: 0.3944, F1 Score: 0.3826\n",
      "Validation - Precision: 0.4029, Recall: 0.3991, F1 Score: 0.3866\n",
      "Validation - Precision: 0.4037, Recall: 0.4035, F1 Score: 0.3914\n",
      "Validation - Precision: 0.4064, Recall: 0.4032, F1 Score: 0.3930\n",
      "Validation - Precision: 0.4123, Recall: 0.4073, F1 Score: 0.3965\n",
      "Validation - Precision: 0.3994, Recall: 0.3976, F1 Score: 0.3851\n"
     ]
    }
   ],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 ë°ì´í„°ì…‹ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ (ì •ê·œí™” ê¸°ì¤€)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ì ìš©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜\n",
    "def objective(trial):\n",
    "    global model, optimizer, scheduler, BATCH_SIZE, LEARNING_RATE, DROPOUT_RATE, WEIGHT_DECAY, best_val_acc, patience_counter\n",
    "\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    model = MLP()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # í•™ìŠµ ê³¼ì •\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validate()  # ê²€ì¦ ë‹¨ê³„\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì „í™˜\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping ì²´í¬\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f\"Best Trial: {study.best_trial.value}\")\n",
    "    print(f\"Best Hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "    # ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ì‹œ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì§„í–‰\n",
    "    load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUJf3vCT46vX"
   },
   "source": [
    "###ğŸ“Œ **GPU ìì› í™œìš©**\n",
    "\n",
    "1. GPU ì„¤ì •\n",
    "\n",
    "```python\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "\n",
    "2. ëª¨ë¸ ì´ˆê¸°í™” í›„ GPUë¡œ ì´ë™(`model.to(DEVICE)`)\n",
    "\n",
    "```python\n",
    "model = MLP().to(DEVICE)\n",
    "```\n",
    "\n",
    "3. ì…ë ¥ ë°ì´í„° (inputs, labels)ë„ GPUë¡œ ì „ì†¡(inputs.to(DEVICE)`)\n",
    "\n",
    "```python\n",
    "inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "```\n",
    "\n",
    "4. ì†ì‹¤ í•¨ìˆ˜ (criterion)ë„ GPUì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ì „ì†¡\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "```\n",
    "\n",
    "5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜ íŒŒíŠ¸ ì•ˆì—ë„ ë˜‘ê°™ì´ ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvPetMoI5Bz8"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 ë°ì´í„°ì…‹ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ (ì •ê·œí™” ê¸°ì¤€)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ì ìš©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        model.to(DEVICE)\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜\n",
    "def objective(trial):\n",
    "    global model, optimizer, scheduler, BATCH_SIZE, LEARNING_RATE, DROPOUT_RATE, WEIGHT_DECAY, best_val_acc, patience_counter\n",
    "\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    model = MLP().to(DEVICE)\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # í•™ìŠµ ê³¼ì •\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validate()  # ê²€ì¦ ë‹¨ê³„\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì „í™˜\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping ì²´í¬\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´ì™€ í™˜ê²½ ì •ë³´ ì €ì¥ í•¨ìˆ˜\n",
    "def save_study_info(study, filename=\"best_study_info.json\"):\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "\n",
    "    # Python ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘\n",
    "    dataset_name = 'CIFAR10'  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ì…‹ ì´ë¦„\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 ì´ë¯¸ì§€ í¬ê¸°\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # ì •ë³´ ì €ì¥\n",
    "    data = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_value\": best_value,\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f\"Best Trial: {study.best_trial.value}\")\n",
    "    print(f\"Best Hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "    # ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ì‹œ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì§„í–‰\n",
    "    load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # ìµœì ì˜ ì •ë³´ ì €ì¥\n",
    "    save_study_info(study)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAxcHNNQvZgg"
   },
   "source": [
    "###ğŸ“Œ **í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ ì €ì¥**\n",
    "\n",
    "1. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´ ì €ì¥\n",
    "- ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° (`best_params`)\n",
    "- ìµœì ì˜ ê²€ì¦ ì •í™•ë„ (`best_value`)\n",
    "\n",
    "2. íŒŒì´ì¬ ë²„ì „ ì €ì¥\n",
    "\n",
    "- `python_version`\n",
    "\n",
    "3. í”Œë«í¼ ì •ë³´ ì €ì¥\n",
    "- `platform_info`\n",
    "\n",
    "4. ì„¤ì¹˜ëœ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë²„ì „ ì •ë³´\n",
    "- `installed_packages`\n",
    "\n",
    "5. ì €ì¥ í˜•ì‹: JSON íŒŒì¼\n",
    "- `import json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUXsEOQvZtl"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 ë°ì´í„°ì…‹ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ (ì •ê·œí™” ê¸°ì¤€)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ì ìš©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„œì¹˜\n",
    "def objective(trial):\n",
    "    global model, optimizer, scheduler, BATCH_SIZE, LEARNING_RATE, DROPOUT_RATE, WEIGHT_DECAY, best_val_acc, patience_counter\n",
    "\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    model = MLP()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # í•™ìŠµ ê³¼ì •\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        if epoch == EPOCHS // 2:\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validate()  # ê²€ì¦ ë‹¨ê³„\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì „í™˜\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping ì²´í¬\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´ì™€ í™˜ê²½ ì •ë³´ ì €ì¥ í•¨ìˆ˜\n",
    "def save_study_info(study, filename=\"best_study_info.json\"):\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "\n",
    "    # Python ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘\n",
    "    dataset_name = 'CIFAR10'  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ì…‹ ì´ë¦„\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 ì´ë¯¸ì§€ í¬ê¸°\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # ì •ë³´ ì €ì¥\n",
    "    data = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_value\": best_value,\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f\"Best Trial: {study.best_trial.value}\")\n",
    "    print(f\"Best Hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "    # ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ì‹œ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì§„í–‰\n",
    "    load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # ìµœì ì˜ ì •ë³´ ì €ì¥\n",
    "    save_study_info(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245640,
     "status": "ok",
     "timestamp": 1743688571439,
     "user": {
      "displayName": "ìœ ê´‘í˜„",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "VPWFnKCS48fB",
    "outputId": "72b193c5-7e13-43dd-d627-5464505d83c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Precision: 0.4357, Recall: 0.4322, F1 Score: 0.4309\n",
      "Epoch [1/20], Train Loss: 1.7769, Val Loss: 1.7876, Val Acc: 43.24%\n",
      "New Best Model Saved! Validation Accuracy: 43.24%\n",
      "Validation - Precision: 0.4652, Recall: 0.4618, F1 Score: 0.4577\n",
      "Epoch [2/20], Train Loss: 1.5758, Val Loss: 1.5997, Val Acc: 46.19%\n",
      "New Best Model Saved! Validation Accuracy: 46.19%\n",
      "Validation - Precision: 0.4862, Recall: 0.4863, F1 Score: 0.4796\n",
      "Epoch [3/20], Train Loss: 1.4930, Val Loss: 1.5601, Val Acc: 48.70%\n",
      "New Best Model Saved! Validation Accuracy: 48.70%\n",
      "Validation - Precision: 0.4958, Recall: 0.4915, F1 Score: 0.4877\n",
      "Epoch [4/20], Train Loss: 1.4359, Val Loss: 1.3529, Val Acc: 49.17%\n",
      "New Best Model Saved! Validation Accuracy: 49.17%\n",
      "Validation - Precision: 0.5065, Recall: 0.5065, F1 Score: 0.5027\n",
      "Epoch [5/20], Train Loss: 1.3889, Val Loss: 1.2718, Val Acc: 50.73%\n",
      "New Best Model Saved! Validation Accuracy: 50.73%\n",
      "Validation - Precision: 0.5260, Recall: 0.5231, F1 Score: 0.5205\n",
      "Epoch [6/20], Train Loss: 1.3067, Val Loss: 1.2284, Val Acc: 52.35%\n",
      "New Best Model Saved! Validation Accuracy: 52.35%\n",
      "Validation - Precision: 0.5279, Recall: 0.5339, F1 Score: 0.5288\n",
      "Epoch [7/20], Train Loss: 1.2685, Val Loss: 1.3184, Val Acc: 53.43%\n",
      "New Best Model Saved! Validation Accuracy: 53.43%\n",
      "Validation - Precision: 0.5399, Recall: 0.5405, F1 Score: 0.5385\n",
      "Epoch [8/20], Train Loss: 1.2436, Val Loss: 1.2941, Val Acc: 54.05%\n",
      "New Best Model Saved! Validation Accuracy: 54.05%\n",
      "Validation - Precision: 0.5387, Recall: 0.5386, F1 Score: 0.5374\n",
      "Epoch [9/20], Train Loss: 1.2178, Val Loss: 1.1353, Val Acc: 53.87%\n",
      "Validation - Precision: 0.5437, Recall: 0.5459, F1 Score: 0.5438\n",
      "Epoch [10/20], Train Loss: 1.1909, Val Loss: 1.1924, Val Acc: 54.61%\n",
      "New Best Model Saved! Validation Accuracy: 54.61%\n",
      "Validation - Precision: 0.5499, Recall: 0.5519, F1 Score: 0.5488\n",
      "Epoch [11/20], Train Loss: 1.1291, Val Loss: 1.2342, Val Acc: 55.14%\n",
      "New Best Model Saved! Validation Accuracy: 55.14%\n",
      "Validation - Precision: 0.5566, Recall: 0.5553, F1 Score: 0.5546\n",
      "Epoch [12/20], Train Loss: 1.1051, Val Loss: 1.0603, Val Acc: 55.57%\n",
      "New Best Model Saved! Validation Accuracy: 55.57%\n",
      "Validation - Precision: 0.5568, Recall: 0.5598, F1 Score: 0.5573\n",
      "Epoch [13/20], Train Loss: 1.0855, Val Loss: 1.1327, Val Acc: 55.99%\n",
      "New Best Model Saved! Validation Accuracy: 55.99%\n",
      "Validation - Precision: 0.5626, Recall: 0.5636, F1 Score: 0.5623\n",
      "Epoch [14/20], Train Loss: 1.0716, Val Loss: 1.1326, Val Acc: 56.38%\n",
      "New Best Model Saved! Validation Accuracy: 56.38%\n",
      "Validation - Precision: 0.5615, Recall: 0.5592, F1 Score: 0.5592\n",
      "Epoch [15/20], Train Loss: 1.0556, Val Loss: 1.0003, Val Acc: 55.94%\n",
      "Validation - Precision: 0.5666, Recall: 0.5642, F1 Score: 0.5640\n",
      "Epoch [16/20], Train Loss: 1.0188, Val Loss: 1.0605, Val Acc: 56.41%\n",
      "New Best Model Saved! Validation Accuracy: 56.41%\n",
      "Validation - Precision: 0.5674, Recall: 0.5667, F1 Score: 0.5659\n",
      "Epoch [17/20], Train Loss: 0.9970, Val Loss: 1.1220, Val Acc: 56.63%\n",
      "New Best Model Saved! Validation Accuracy: 56.63%\n",
      "Validation - Precision: 0.5655, Recall: 0.5641, F1 Score: 0.5640\n",
      "Epoch [18/20], Train Loss: 0.9889, Val Loss: 1.1030, Val Acc: 56.42%\n",
      "Validation - Precision: 0.5698, Recall: 0.5676, F1 Score: 0.5676\n",
      "Epoch [19/20], Train Loss: 0.9788, Val Loss: 1.0682, Val Acc: 56.74%\n",
      "New Best Model Saved! Validation Accuracy: 56.74%\n",
      "Validation - Precision: 0.5703, Recall: 0.5700, F1 Score: 0.5698\n",
      "Epoch [20/20], Train Loss: 0.9749, Val Loss: 1.0967, Val Acc: 57.02%\n",
      "New Best Model Saved! Validation Accuracy: 57.02%\n",
      "Test Accuracy: 57.40%\n",
      "Test Precision: 0.5735, Recall: 0.5740, F1 Score: 0.5734\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 ë°ì´í„°ì…‹ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ (ì •ê·œí™” ê¸°ì¤€)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ì¦ê°• ì„¤ì • (í›„ë°˜ í•™ìŠµìš©)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ì ìš©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping ì²´í¬\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # ë°˜í™˜:(ìµœëŒ“ê°’, ì¸ë±ìŠ¤)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´ì™€ í™˜ê²½ ì •ë³´ ì €ì¥ í•¨ìˆ˜\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘\n",
    "    dataset_name = 'CIFAR10'  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ì…‹ ì´ë¦„\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 ì´ë¯¸ì§€ í¬ê¸°\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # ì •ë³´ ì €ì¥\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # ìµœì ì˜ ì •ë³´ ì €ì¥\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlWGqvZNnC1E"
   },
   "source": [
    "###ğŸ“Œ **CUSTOM DATASET**\n",
    "\n",
    "1. `CustomDataset` í´ë˜ìŠ¤ ì¶”ê°€:\n",
    "- ì§€ì •ëœ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í´ë” êµ¬ì¡°ë¡œ ë¡œë“œ\n",
    "- í´ë” ì´ë¦„ì´ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ê°„ì£¼ (ì˜ˆ: ./data/train/class1, ./data/train/class2 ë“±)\n",
    "- ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì: .png, .jpg, .jpeg\n",
    "\n",
    " ğŸ“Œ **ì‚¬ìš© ë°©ë²•**\n",
    "- í•™ìŠµ ë°ì´í„° ê²½ë¡œ: `./data/train`\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: `./data/test`\n",
    "- **í´ë” êµ¬ì¡°**ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "```bash\n",
    "./data/train/class1/ ì´ë¯¸ì§€1.jpg, ì´ë¯¸ì§€2.jpg, ...\n",
    "./data/train/class2/ ì´ë¯¸ì§€1.jpg, ì´ë¯¸ì§€2.jpg, ...\n",
    "./data/test/class1/ ì´ë¯¸ì§€1.jpg, ì´ë¯¸ì§€2.jpg, ...\n",
    "./data/test/class2/ ì´ë¯¸ì§€1.jpg, ì´ë¯¸ì§€2.jpg, ...\n",
    "```\n",
    "\n",
    "2. ë³´í¸ì  ì •ê·œí™” ê°’ ì ìš©:\n",
    "\n",
    "```python\n",
    "CUSTOM_MEAN = (0.5, 0.5, 0.5)\n",
    "CUSTOM_STD = (0.5, 0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K5t3H52nDTX"
   },
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ì •ê·œí™” í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì„¤ì •\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "# ìì²´ ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ì •ê·œí™” í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì„¤ì •\n",
    "CUSTOM_MEAN = (0.5, 0.5, 0.5)\n",
    "CUSTOM_STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "# ì•½í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (ì´ˆê¸° í•™ìŠµìš©)\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CUSTOM_MEAN, CUSTOM_STD)\n",
    "])\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• ì„¤ì • (í›„ë°˜ í•™ìŠµìš©)\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CUSTOM_MEAN, CUSTOM_STD)\n",
    "])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ì ìš©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CUSTOM_MEAN, CUSTOM_STD)\n",
    "])\n",
    "\n",
    "# ìì²´ ë°ì´í„°ì…‹ ë¡œë“œ í´ë˜ìŠ¤ ì •ì˜\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        classes = os.listdir(root_dir)  # í´ë” ì´ë¦„ì„ í´ë˜ìŠ¤ë¡œ ì‚¬ìš©\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root_dir, cls_name)\n",
    "            if os.path.isdir(cls_folder):\n",
    "                for img_name in os.listdir(cls_folder):\n",
    "                    img_path = os.path.join(cls_folder, img_name)\n",
    "                    if img_path.endswith(('.png', '.jpg', '.jpeg')):  # ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì¶”ê°€\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "train_dir = './data/train'\n",
    "test_dir = './data/test'\n",
    "\n",
    "# ìì²´ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_train_dataset = CustomDataset(root_dir=train_dir, transform=weak_transform)\n",
    "test_dataset = CustomDataset(root_dir=test_dir, transform=transform_test)\n",
    "\n",
    "# full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=weak_transform)\n",
    "# test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = MLP()\n",
    "\n",
    "# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == EPOCHS // 2:  # ì—í­ ì ˆë°˜ ì´í›„ ê°•í•œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì „í™˜\n",
    "            train_dataset.dataset.transform = strong_transform\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping ì²´í¬\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()  # ëª¨ë¸ ë¡œë“œ ì‹œë„\n",
    "    train()\n",
    "    test()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZMpivB4aIIuUZllY3NiWj",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
