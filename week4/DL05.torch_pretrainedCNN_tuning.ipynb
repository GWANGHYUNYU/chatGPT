{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU9C9STY9odn"
   },
   "source": [
    "## 🧠 Pretrained ResNet18\n",
    "- 🔧 필요한 import 추가\n",
    "\n",
    "```python\n",
    "from torchvision.models import resnet18\n",
    "```\n",
    "\n",
    "- 🧠 모델 생성 함수 만들기 (사전학습 가중치 사용)\n",
    "\n",
    "```python\n",
    "def build_pretrained_resnet18(num_classes=10):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # 선택: CIFAR에 맞게 conv1 수정 (안 해도 동작은 함)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    return model.to(DEVICE)\n",
    "```\n",
    "\n",
    "- 🧠 전이학습 옵션\n",
    "\n",
    "```python\n",
    "# 1. 모델 정의\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "# 2. 출력 레이어 수정 (1000 → 10)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "# 3. 파라미터 학습 여부 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # 전체 동결 (freeze)\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True   # fc만 학습 가능\n",
    "\n",
    "# 4. 모델 디바이스로 이동\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141096,
     "status": "ok",
     "timestamp": 1744121479737,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "cly53L9M9ox8",
    "outputId": "72528737-10ec-4d78-c7df-28cd68d58a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-acf9fcea670d>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "100%|██████████| 170M/170M [00:02<00:00, 68.6MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 176MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [128, 10]                 --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         (1,728)\n",
      "├─BatchNorm2d: 1-2                       [128, 64, 32, 32]         (128)\n",
      "├─ReLU: 1-3                              [128, 64, 32, 32]         --\n",
      "├─Identity: 1-4                          [128, 64, 32, 32]         --\n",
      "├─Sequential: 1-5                        [128, 64, 32, 32]         --\n",
      "│    └─BasicBlock: 2-1                   [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-2             [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-3                    [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-4                  [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-5             [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-6                    [128, 64, 32, 32]         --\n",
      "│    └─BasicBlock: 2-2                   [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-7                  [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-8             [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-9                    [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-10                 [128, 64, 32, 32]         (36,864)\n",
      "│    │    └─BatchNorm2d: 3-11            [128, 64, 32, 32]         (128)\n",
      "│    │    └─ReLU: 3-12                   [128, 64, 32, 32]         --\n",
      "├─Sequential: 1-6                        [128, 128, 16, 16]        --\n",
      "│    └─BasicBlock: 2-3                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-13                 [128, 128, 16, 16]        (73,728)\n",
      "│    │    └─BatchNorm2d: 3-14            [128, 128, 16, 16]        (256)\n",
      "│    │    └─ReLU: 3-15                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-16                 [128, 128, 16, 16]        (147,456)\n",
      "│    │    └─BatchNorm2d: 3-17            [128, 128, 16, 16]        (256)\n",
      "│    │    └─Sequential: 3-18             [128, 128, 16, 16]        (8,448)\n",
      "│    │    └─ReLU: 3-19                   [128, 128, 16, 16]        --\n",
      "│    └─BasicBlock: 2-4                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-20                 [128, 128, 16, 16]        (147,456)\n",
      "│    │    └─BatchNorm2d: 3-21            [128, 128, 16, 16]        (256)\n",
      "│    │    └─ReLU: 3-22                   [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-23                 [128, 128, 16, 16]        (147,456)\n",
      "│    │    └─BatchNorm2d: 3-24            [128, 128, 16, 16]        (256)\n",
      "│    │    └─ReLU: 3-25                   [128, 128, 16, 16]        --\n",
      "├─Sequential: 1-7                        [128, 256, 8, 8]          --\n",
      "│    └─BasicBlock: 2-5                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-26                 [128, 256, 8, 8]          (294,912)\n",
      "│    │    └─BatchNorm2d: 3-27            [128, 256, 8, 8]          (512)\n",
      "│    │    └─ReLU: 3-28                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-29                 [128, 256, 8, 8]          (589,824)\n",
      "│    │    └─BatchNorm2d: 3-30            [128, 256, 8, 8]          (512)\n",
      "│    │    └─Sequential: 3-31             [128, 256, 8, 8]          (33,280)\n",
      "│    │    └─ReLU: 3-32                   [128, 256, 8, 8]          --\n",
      "│    └─BasicBlock: 2-6                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-33                 [128, 256, 8, 8]          (589,824)\n",
      "│    │    └─BatchNorm2d: 3-34            [128, 256, 8, 8]          (512)\n",
      "│    │    └─ReLU: 3-35                   [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-36                 [128, 256, 8, 8]          (589,824)\n",
      "│    │    └─BatchNorm2d: 3-37            [128, 256, 8, 8]          (512)\n",
      "│    │    └─ReLU: 3-38                   [128, 256, 8, 8]          --\n",
      "├─Sequential: 1-8                        [128, 512, 4, 4]          --\n",
      "│    └─BasicBlock: 2-7                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-39                 [128, 512, 4, 4]          (1,179,648)\n",
      "│    │    └─BatchNorm2d: 3-40            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─ReLU: 3-41                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-42                 [128, 512, 4, 4]          (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-43            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─Sequential: 3-44             [128, 512, 4, 4]          (132,096)\n",
      "│    │    └─ReLU: 3-45                   [128, 512, 4, 4]          --\n",
      "│    └─BasicBlock: 2-8                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-46                 [128, 512, 4, 4]          (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-47            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─ReLU: 3-48                   [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-49                 [128, 512, 4, 4]          (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-50            [128, 512, 4, 4]          (1,024)\n",
      "│    │    └─ReLU: 3-51                   [128, 512, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [128, 512, 1, 1]          --\n",
      "├─Linear: 1-10                           [128, 10]                 5,130\n",
      "==========================================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 11,168,832\n",
      "Total mult-adds (Units.GIGABYTES): 71.10\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 1258.30\n",
      "Params size (MB): 44.70\n",
      "Estimated Total Size (MB): 1304.57\n",
      "==========================================================================================\n",
      "Validation - Precision: 0.4700, Recall: 0.4702, F1 Score: 0.4670\n",
      "Epoch [1/10], Train Loss: 1.6779, Val Loss: 1.5101, Val Acc: 47.06%\n",
      "New Best Model Saved! Validation Accuracy: 47.06%\n",
      "Validation - Precision: 0.4865, Recall: 0.4809, F1 Score: 0.4745\n",
      "Epoch [2/10], Train Loss: 1.4405, Val Loss: 1.4654, Val Acc: 48.13%\n",
      "New Best Model Saved! Validation Accuracy: 48.13%\n",
      "Validation - Precision: 0.4999, Recall: 0.4886, F1 Score: 0.4893\n",
      "Epoch [3/10], Train Loss: 1.3963, Val Loss: 1.4472, Val Acc: 48.79%\n",
      "New Best Model Saved! Validation Accuracy: 48.79%\n",
      "Validation - Precision: 0.5097, Recall: 0.4998, F1 Score: 0.4997\n",
      "Epoch [4/10], Train Loss: 1.3762, Val Loss: 1.4356, Val Acc: 50.02%\n",
      "New Best Model Saved! Validation Accuracy: 50.02%\n",
      "Validation - Precision: 0.4944, Recall: 0.4964, F1 Score: 0.4915\n",
      "Epoch [5/10], Train Loss: 1.3664, Val Loss: 1.4244, Val Acc: 49.66%\n",
      "Validation - Precision: 0.5027, Recall: 0.5045, F1 Score: 0.5011\n",
      "Epoch [6/10], Train Loss: 1.3348, Val Loss: 1.4218, Val Acc: 50.42%\n",
      "New Best Model Saved! Validation Accuracy: 50.42%\n",
      "Validation - Precision: 0.5052, Recall: 0.5026, F1 Score: 0.5033\n",
      "Epoch [7/10], Train Loss: 1.3308, Val Loss: 1.4148, Val Acc: 50.28%\n",
      "Validation - Precision: 0.5068, Recall: 0.5021, F1 Score: 0.4998\n",
      "Epoch [8/10], Train Loss: 1.3281, Val Loss: 1.4240, Val Acc: 50.24%\n",
      "Validation - Precision: 0.5072, Recall: 0.5029, F1 Score: 0.5028\n",
      "Epoch [9/10], Train Loss: 1.3251, Val Loss: 1.4180, Val Acc: 50.35%\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 51.13%\n",
      "Test Precision: 0.5105, Recall: 0.5113, F1 Score: 0.5078\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchinfo import summary\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# GPU 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 학습 데이터 증강 설정 (후반 학습용)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 테스트 데이터 변환 (데이터 증강 없이 정규화만 적용)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분리\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 불러오기 (ResNet18)\n",
    "def build_pretrained_resnet18(num_classes=10):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # 선택: CIFAR에 맞게 conv1 수정 (안 해도 동작은 함)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    # 파라미터 학습 여부 설정\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # 전체 동결 (freeze)\n",
    "\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True   # fc만 학습 가능\n",
    "\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# 모델 초기화\n",
    "model = build_pretrained_resnet18(num_classes=NUM_CLASSES)\n",
    "\n",
    "# summary 호출\n",
    "print(summary(model, input_size=(BATCH_SIZE, 3, 32, 32)))\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping 체크\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # 반환:(최댓값, 인덱스)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 정보와 환경 정보 저장 함수\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python 및 라이브러리 정보 가져오기\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # 데이터셋 정보 수집\n",
    "    dataset_name = 'CIFAR10'  # 현재 사용 중인 데이터셋 이름\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 이미지 크기\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # 정보 저장\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    # load_model()\n",
    "    test()\n",
    "\n",
    "    # 최적의 정보 저장\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaSEXLxYvFCq"
   },
   "source": [
    "## A. 베이스라인 구성 요약\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|Adam|\n",
    "|Learning Rate|\t0.001 (고정)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\t기본 (Flip + Crop)|\n",
    "|정규화|\t없음 (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246425,
     "status": "ok",
     "timestamp": 1744122603120,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "lBpIDCUkxDe-",
    "outputId": "62ae85b5-4b0a-492b-c0df-caed8fef9146"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.0183 | Test Loss: 0.7654 | Acc: 74.02% | F1: 0.7408 | Precision: 0.7528 | Recall: 0.7402\n",
      "Best model saved!\n",
      "[Epoch 2/10] Train Loss: 0.7371 | Test Loss: 0.7276 | Acc: 75.77% | F1: 0.7539 | Precision: 0.7663 | Recall: 0.7577\n",
      "Best model saved!\n",
      "[Epoch 3/10] Train Loss: 0.6515 | Test Loss: 0.6653 | Acc: 77.02% | F1: 0.7701 | Precision: 0.7792 | Recall: 0.7702\n",
      "Best model saved!\n",
      "[Epoch 4/10] Train Loss: 0.6015 | Test Loss: 0.6450 | Acc: 78.28% | F1: 0.7842 | Precision: 0.7988 | Recall: 0.7828\n",
      "Best model saved!\n",
      "[Epoch 5/10] Train Loss: 0.5519 | Test Loss: 0.5892 | Acc: 79.65% | F1: 0.7983 | Precision: 0.8079 | Recall: 0.7965\n",
      "Best model saved!\n",
      "[Epoch 6/10] Train Loss: 0.5183 | Test Loss: 0.5563 | Acc: 80.73% | F1: 0.8067 | Precision: 0.8140 | Recall: 0.8073\n",
      "Best model saved!\n",
      "[Epoch 7/10] Train Loss: 0.5034 | Test Loss: 0.5774 | Acc: 79.97% | F1: 0.8010 | Precision: 0.8100 | Recall: 0.7997\n",
      "[Epoch 8/10] Train Loss: 0.4777 | Test Loss: 0.5386 | Acc: 82.16% | F1: 0.8207 | Precision: 0.8243 | Recall: 0.8216\n",
      "Best model saved!\n",
      "[Epoch 9/10] Train Loss: 0.4448 | Test Loss: 0.5377 | Acc: 81.90% | F1: 0.8177 | Precision: 0.8270 | Recall: 0.8190\n",
      "[Epoch 10/10] Train Loss: 0.4275 | Test Loss: 0.5417 | Acc: 81.75% | F1: 0.8179 | Precision: 0.8234 | Recall: 0.8175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_A.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run Baseline Experiment\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP5UpXhhz14N"
   },
   "source": [
    "## B. OneCycleLR 적용\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|Adam|\n",
    "|Learning Rate|\tOneCycleLR(max_lr=0.01)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\t기본 (Flip + Crop)|\n",
    "|정규화|\t없음 (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|\n",
    "\n",
    "```\n",
    "lr\n",
    "│     ▲\n",
    "│    /\\\\           max_lr = 0.01\n",
    "│   /  \\\\__        \n",
    "│  /     \\\\     ↓ 감소구간\n",
    "│_/       \\\\_____________ final_lr ≈ 1e-6\n",
    "│\n",
    "└────────────────────▶ steps\n",
    "  ^      ^           ^\n",
    "  |      |           |\n",
    "  시작    최고점       종료\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245216,
     "status": "ok",
     "timestamp": 1744123575700,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "_-XRQ-GHz2Nj",
    "outputId": "342b8f15-d826-400c-dc35-685f3074d3b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.3622 | Test Loss: 0.8592 | Acc: 70.50% | F1: 0.7028 | Precision: 0.7098 | Recall: 0.7050\n",
      "✅ Best model saved!\n",
      "[Epoch 2/10] Train Loss: 0.8192 | Test Loss: 0.7642 | Acc: 73.81% | F1: 0.7363 | Precision: 0.7452 | Recall: 0.7381\n",
      "✅ Best model saved!\n",
      "[Epoch 3/10] Train Loss: 0.7255 | Test Loss: 0.6921 | Acc: 76.64% | F1: 0.7617 | Precision: 0.7735 | Recall: 0.7664\n",
      "✅ Best model saved!\n",
      "[Epoch 4/10] Train Loss: 0.6532 | Test Loss: 0.6135 | Acc: 79.36% | F1: 0.7973 | Precision: 0.8096 | Recall: 0.7936\n",
      "✅ Best model saved!\n",
      "[Epoch 5/10] Train Loss: 0.5697 | Test Loss: 0.6204 | Acc: 79.63% | F1: 0.7982 | Precision: 0.8063 | Recall: 0.7963\n",
      "✅ Best model saved!\n",
      "[Epoch 6/10] Train Loss: 0.5071 | Test Loss: 0.5041 | Acc: 83.11% | F1: 0.8295 | Precision: 0.8305 | Recall: 0.8311\n",
      "✅ Best model saved!\n",
      "[Epoch 7/10] Train Loss: 0.4385 | Test Loss: 0.4667 | Acc: 84.12% | F1: 0.8403 | Precision: 0.8415 | Recall: 0.8412\n",
      "✅ Best model saved!\n",
      "[Epoch 8/10] Train Loss: 0.3681 | Test Loss: 0.4299 | Acc: 85.54% | F1: 0.8550 | Precision: 0.8555 | Recall: 0.8554\n",
      "✅ Best model saved!\n",
      "[Epoch 9/10] Train Loss: 0.3180 | Test Loss: 0.4055 | Acc: 86.47% | F1: 0.8639 | Precision: 0.8638 | Recall: 0.8647\n",
      "✅ Best model saved!\n",
      "[Epoch 10/10] Train Loss: 0.2781 | Test Loss: 0.4033 | Acc: 86.47% | F1: 0.8641 | Precision: 0.8641 | Recall: 0.8647\n",
      "✅ Best model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_B.csv\"\n",
    "EXPERIMENT_NAME = \"B_onecycle\"\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run OneCycleLR Experiment (B)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LEARNING_RATE, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz9VB4XP2x8A"
   },
   "source": [
    "## C. Mixup 적용\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|Adam|\n",
    "|Learning Rate|\tOneCycleLR(max_lr=0.01)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\t기본 (Flip + Crop), mixup(alpha=0.2)|\n",
    "|정규화|\t없음 (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|\n",
    "\n",
    "### 🌀 Mixup이란?\n",
    "두 개의 이미지와 라벨을 선형 조합하여 모델에 학습시키는 방법.\n",
    "- **\"x = lam·x₁ + (1-lam)·x₂\"**\n",
    "- **\"y = lam·y₁ + (1-lam)·y₂\"**\n",
    "\n",
    "```\n",
    "입력 이미지 A (개)                   입력 이미지 B (고양이)\n",
    "        │                                     │\n",
    "        └────┬────lam────────┘\n",
    "                  ▼\n",
    "        섞인 이미지 (개 + 고양이)\n",
    "\n",
    "정답 라벨: 개 70%, 고양이 30%\n",
    "```\n",
    "\n",
    "이렇게 학습하면:\n",
    "- 결정 경계가 더 부드럽고 안정적\n",
    "- 과적합 ↓, 일반화 ↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244970,
     "status": "ok",
     "timestamp": 1744124005051,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "j4eBXzAm3QfT",
    "outputId": "0ace2b85-273e-496e-8308-e3a062cb5d2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.4111 | Test Loss: 1.2367 | Acc: 57.49% | F1: 0.5513 | Precision: 0.6117 | Recall: 0.5749\n",
      "✅ Best model saved!\n",
      "[Epoch 2/10] Train Loss: 1.4346 | Test Loss: 1.1424 | Acc: 62.21% | F1: 0.6093 | Precision: 0.6670 | Recall: 0.6221\n",
      "✅ Best model saved!\n",
      "[Epoch 3/10] Train Loss: 1.3829 | Test Loss: 1.1948 | Acc: 59.01% | F1: 0.5741 | Precision: 0.6248 | Recall: 0.5901\n",
      "[Epoch 4/10] Train Loss: 1.2835 | Test Loss: 1.0320 | Acc: 65.40% | F1: 0.6483 | Precision: 0.6910 | Recall: 0.6540\n",
      "✅ Best model saved!\n",
      "[Epoch 5/10] Train Loss: 1.1021 | Test Loss: 0.8928 | Acc: 69.10% | F1: 0.6821 | Precision: 0.7107 | Recall: 0.6910\n",
      "✅ Best model saved!\n",
      "[Epoch 6/10] Train Loss: 1.0799 | Test Loss: 0.7293 | Acc: 76.16% | F1: 0.7591 | Precision: 0.7662 | Recall: 0.7616\n",
      "✅ Best model saved!\n",
      "[Epoch 7/10] Train Loss: 0.9969 | Test Loss: 0.6409 | Acc: 78.71% | F1: 0.7848 | Precision: 0.7935 | Recall: 0.7871\n",
      "✅ Best model saved!\n",
      "[Epoch 8/10] Train Loss: 0.9742 | Test Loss: 0.6048 | Acc: 80.65% | F1: 0.8064 | Precision: 0.8099 | Recall: 0.8065\n",
      "✅ Best model saved!\n",
      "[Epoch 9/10] Train Loss: 0.8852 | Test Loss: 0.5602 | Acc: 82.57% | F1: 0.8245 | Precision: 0.8247 | Recall: 0.8257\n",
      "✅ Best model saved!\n",
      "[Epoch 10/10] Train Loss: 0.8614 | Test Loss: 0.5447 | Acc: 82.39% | F1: 0.8225 | Precision: 0.8230 | Recall: 0.8239\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_C.csv\"\n",
    "EXPERIMENT_NAME = \"C_mixup\"\n",
    "ALPHA = 0.2  # mixup alpha value\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Mixup Function\n",
    "# -----------------------------\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(DEVICE)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        x, y_a, y_b, lam = mixup_data(x, y, ALPHA)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run Mixup Experiment (C)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGtzUap-4Lq8"
   },
   "source": [
    "## D. Label Smoothing 적용\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|Adam|\n",
    "|Learning Rate|\tOneCycleLR(max_lr=0.01)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\t기본 (Flip + Crop), mixup(alpha=0.2)|\n",
    "|정규화|\tLabel Smoothing (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|\n",
    "\n",
    "✅ Label Smoothing이란?\n",
    "- 정답 라벨을 100% 확신하지 않도록 부드럽게 만드는 기법.\n",
    "\n",
    "|클래스\t|One-hot 라벨|\t모델 출력|\n",
    "|---|---|---|\n",
    "|개|\t1|\t0.80|\n",
    "|고양이\t|0|\t0.10|\n",
    "|말\t|0\t|0.10|\n",
    "\n",
    "\n",
    "|클래스|\tSmoothed 라벨 (α=0.1)|\n",
    "|---|---|\n",
    "|개|\t0.90|\n",
    "|고양이\t|0.05|\n",
    "|말\t|0.05|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250855,
     "status": "ok",
     "timestamp": 1744124574568,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "UmF154h84MBV",
    "outputId": "95cf3727-22ce-4b62-d274-8035f0cade8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.3576 | Test Loss: 1.3693 | Acc: 63.95% | F1: 0.6413 | Precision: 0.6657 | Recall: 0.6395\n",
      "✅ Best model saved!\n",
      "[Epoch 2/10] Train Loss: 1.3486 | Test Loss: 3.9348 | Acc: 29.08% | F1: 0.2557 | Precision: 0.3846 | Recall: 0.2908\n",
      "[Epoch 3/10] Train Loss: 1.4065 | Test Loss: 1.2390 | Acc: 68.35% | F1: 0.6796 | Precision: 0.6866 | Recall: 0.6835\n",
      "✅ Best model saved!\n",
      "[Epoch 4/10] Train Loss: 1.1620 | Test Loss: 1.1853 | Acc: 69.84% | F1: 0.7060 | Precision: 0.7312 | Recall: 0.6984\n",
      "✅ Best model saved!\n",
      "[Epoch 5/10] Train Loss: 1.0849 | Test Loss: 1.0826 | Acc: 74.36% | F1: 0.7431 | Precision: 0.7574 | Recall: 0.7436\n",
      "✅ Best model saved!\n",
      "[Epoch 6/10] Train Loss: 1.0195 | Test Loss: 1.0252 | Acc: 77.17% | F1: 0.7644 | Precision: 0.7809 | Recall: 0.7717\n",
      "✅ Best model saved!\n",
      "[Epoch 7/10] Train Loss: 0.9572 | Test Loss: 0.9657 | Acc: 79.43% | F1: 0.7919 | Precision: 0.7950 | Recall: 0.7943\n",
      "✅ Best model saved!\n",
      "[Epoch 8/10] Train Loss: 1.0420 | Test Loss: 0.9752 | Acc: 79.21% | F1: 0.7885 | Precision: 0.7902 | Recall: 0.7921\n",
      "[Epoch 9/10] Train Loss: 0.9310 | Test Loss: 0.9210 | Acc: 81.33% | F1: 0.8122 | Precision: 0.8122 | Recall: 0.8133\n",
      "✅ Best model saved!\n",
      "[Epoch 10/10] Train Loss: 0.8898 | Test Loss: 0.9126 | Acc: 81.63% | F1: 0.8154 | Precision: 0.8153 | Recall: 0.8163\n",
      "✅ Best model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_D.csv\"\n",
    "EXPERIMENT_NAME = \"D_labelsmoothing\"\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run Label Smoothing Experiment (D)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL4gJipy6z1D"
   },
   "source": [
    "## E. CutMix 적용\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|Adam|\n",
    "|Learning Rate|\tOneCycleLR(max_lr=0.01)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\t기본 (Flip + Crop), CutMix(alpha=1.0)|\n",
    "|정규화|\t없음 (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248957,
     "status": "ok",
     "timestamp": 1744125234684,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "4UH3tJNK7kiq",
    "outputId": "a9257888-60e7-4c6e-9e0e-9855fb776f6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.8268 | Test Loss: 1.2928 | Acc: 56.02% | F1: 0.5430 | Precision: 0.6090 | Recall: 0.5602\n",
      "✅ Best model saved!\n",
      "[Epoch 2/10] Train Loss: 1.8932 | Test Loss: 1.6965 | Acc: 38.60% | F1: 0.3407 | Precision: 0.4789 | Recall: 0.3860\n",
      "[Epoch 3/10] Train Loss: 1.9342 | Test Loss: 1.4444 | Acc: 49.72% | F1: 0.4828 | Precision: 0.5360 | Recall: 0.4972\n",
      "[Epoch 4/10] Train Loss: 1.7837 | Test Loss: 1.1898 | Acc: 59.85% | F1: 0.5901 | Precision: 0.6144 | Recall: 0.5985\n",
      "✅ Best model saved!\n",
      "[Epoch 5/10] Train Loss: 1.6941 | Test Loss: 1.0709 | Acc: 63.83% | F1: 0.6360 | Precision: 0.6563 | Recall: 0.6383\n",
      "✅ Best model saved!\n",
      "[Epoch 6/10] Train Loss: 1.6412 | Test Loss: 1.0488 | Acc: 67.15% | F1: 0.6705 | Precision: 0.6914 | Recall: 0.6715\n",
      "✅ Best model saved!\n",
      "[Epoch 7/10] Train Loss: 1.5801 | Test Loss: 0.8864 | Acc: 71.05% | F1: 0.7101 | Precision: 0.7184 | Recall: 0.7105\n",
      "✅ Best model saved!\n",
      "[Epoch 8/10] Train Loss: 1.5559 | Test Loss: 0.8280 | Acc: 73.12% | F1: 0.7298 | Precision: 0.7344 | Recall: 0.7312\n",
      "✅ Best model saved!\n",
      "[Epoch 9/10] Train Loss: 1.4858 | Test Loss: 0.7837 | Acc: 75.50% | F1: 0.7540 | Precision: 0.7565 | Recall: 0.7550\n",
      "✅ Best model saved!\n",
      "[Epoch 10/10] Train Loss: 1.4706 | Test Loss: 0.8009 | Acc: 75.58% | F1: 0.7546 | Precision: 0.7580 | Recall: 0.7558\n",
      "✅ Best model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_E.csv\"\n",
    "EXPERIMENT_NAME = \"E_cutmix\"\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# CutMix Function\n",
    "# -----------------------------\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        indices = torch.randperm(x.size(0)).to(DEVICE)\n",
    "        x2, y2 = x[indices], y[indices]\n",
    "        lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
    "\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "        x[:, :, bby1:bby2, bbx1:bbx2] = x2[:, :, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "        # Adjust lambda based on the area of the cut region\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = lam * criterion(outputs, y) + (1 - lam) * criterion(outputs, y2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run CutMix Experiment (E)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7drNmMZ9P7Q"
   },
   "source": [
    "## F. AdamW 적용\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|AdamW|\n",
    "|Learning Rate|\tOneCycleLR(max_lr=0.01)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\t기본 (Flip + Crop)|\n",
    "|정규화|\tLabel Smoothing (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249833,
     "status": "ok",
     "timestamp": 1744125528668,
     "user": {
      "displayName": "유광현",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "Ge2wzS5U9QP2",
    "outputId": "920a977d-408b-4a8d-8415-dacaa2d8d60f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.3584 | Test Loss: 1.5930 | Acc: 54.48% | F1: 0.5469 | Precision: 0.6637 | Recall: 0.5448\n",
      "✅ Best model saved!\n",
      "[Epoch 2/10] Train Loss: 1.3926 | Test Loss: 1.4346 | Acc: 59.31% | F1: 0.5895 | Precision: 0.5964 | Recall: 0.5931\n",
      "✅ Best model saved!\n",
      "[Epoch 3/10] Train Loss: 1.2823 | Test Loss: 1.1763 | Acc: 70.65% | F1: 0.7077 | Precision: 0.7196 | Recall: 0.7065\n",
      "✅ Best model saved!\n",
      "[Epoch 4/10] Train Loss: 1.1429 | Test Loss: 1.2107 | Acc: 70.34% | F1: 0.6991 | Precision: 0.7245 | Recall: 0.7034\n",
      "[Epoch 5/10] Train Loss: 1.0784 | Test Loss: 1.1105 | Acc: 73.47% | F1: 0.7337 | Precision: 0.7560 | Recall: 0.7347\n",
      "✅ Best model saved!\n",
      "[Epoch 6/10] Train Loss: 1.0152 | Test Loss: 0.9805 | Acc: 78.77% | F1: 0.7851 | Precision: 0.7880 | Recall: 0.7877\n",
      "✅ Best model saved!\n",
      "[Epoch 7/10] Train Loss: 0.9585 | Test Loss: 0.9357 | Acc: 80.82% | F1: 0.8093 | Precision: 0.8116 | Recall: 0.8082\n",
      "✅ Best model saved!\n",
      "[Epoch 8/10] Train Loss: 0.8931 | Test Loss: 0.8930 | Acc: 82.83% | F1: 0.8263 | Precision: 0.8305 | Recall: 0.8283\n",
      "✅ Best model saved!\n",
      "[Epoch 9/10] Train Loss: 0.8347 | Test Loss: 0.8441 | Acc: 84.70% | F1: 0.8460 | Precision: 0.8459 | Recall: 0.8470\n",
      "✅ Best model saved!\n",
      "[Epoch 10/10] Train Loss: 0.7991 | Test Loss: 0.8376 | Acc: 84.96% | F1: 0.8488 | Precision: 0.8492 | Recall: 0.8496\n",
      "✅ Best model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_F.csv\"\n",
    "EXPERIMENT_NAME = \"F_adamw\"\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run AdamW Optimizer Experiment (F)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jt-qf7-9_wV"
   },
   "source": [
    "## G. AutoAugment 적용\n",
    "\n",
    "| 항목 |\t내용 |\n",
    "|---|---|\n",
    "|모델|pretrained ResNet18 + fc 수정|\n",
    "|Optimizer|AdamW|\n",
    "|Learning Rate|\tOneCycleLR(max_lr=0.01)|\n",
    "|Epochs|\t10|\n",
    "|데이터 증강|\tAutoAugment(CIFAR10) |\n",
    "|정규화|\tLabel Smoothing (기본 CrossEntropyLoss)|\n",
    "|평가지표|accuracy, f1-score, precision, recall|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "hN08QH4Y-YZR",
    "outputId": "90a5129d-6f78-4647-f0c9-ca84194ba8ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 1.4926 | Test Loss: 1.5395 | Acc: 56.55% | F1: 0.5684 | Precision: 0.6765 | Recall: 0.5655\n",
      "✅ Best model saved!\n",
      "[Epoch 2/10] Train Loss: 1.5703 | Test Loss: 1.5757 | Acc: 51.99% | F1: 0.5074 | Precision: 0.5470 | Recall: 0.5199\n",
      "[Epoch 3/10] Train Loss: 1.5119 | Test Loss: 1.2668 | Acc: 67.03% | F1: 0.6636 | Precision: 0.6935 | Recall: 0.6703\n",
      "✅ Best model saved!\n",
      "[Epoch 4/10] Train Loss: 1.3411 | Test Loss: 1.1787 | Acc: 70.80% | F1: 0.7051 | Precision: 0.7335 | Recall: 0.7080\n",
      "✅ Best model saved!\n",
      "[Epoch 5/10] Train Loss: 1.2425 | Test Loss: 1.1014 | Acc: 73.81% | F1: 0.7317 | Precision: 0.7420 | Recall: 0.7381\n",
      "✅ Best model saved!\n",
      "[Epoch 6/10] Train Loss: 1.1586 | Test Loss: 1.0365 | Acc: 76.63% | F1: 0.7630 | Precision: 0.7728 | Recall: 0.7663\n",
      "✅ Best model saved!\n",
      "[Epoch 7/10] Train Loss: 1.0903 | Test Loss: 0.9882 | Acc: 78.41% | F1: 0.7875 | Precision: 0.7984 | Recall: 0.7841\n",
      "✅ Best model saved!\n",
      "[Epoch 8/10] Train Loss: 0.9971 | Test Loss: 0.9323 | Acc: 81.03% | F1: 0.8096 | Precision: 0.8120 | Recall: 0.8103\n",
      "✅ Best model saved!\n",
      "[Epoch 9/10] Train Loss: 0.9098 | Test Loss: 0.9014 | Acc: 82.43% | F1: 0.8241 | Precision: 0.8245 | Recall: 0.8243\n",
      "✅ Best model saved!\n",
      "[Epoch 10/10] Train Loss: 0.8532 | Test Loss: 0.9002 | Acc: 82.67% | F1: 0.8266 | Precision: 0.8267 | Recall: 0.8267\n",
      "✅ Best model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "CSV_LOG = \"experiment_results_G.csv\"\n",
    "EXPERIMENT_NAME = \"G_autoaugment\"\n",
    "\n",
    "# -----------------------------\n",
    "# Data Transforms & Loaders\n",
    "# -----------------------------\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "def build_baseline_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train(model, loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item() * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), acc, f1, precision, recall\n",
    "\n",
    "# -----------------------------\n",
    "# Save CSV Log\n",
    "# -----------------------------\n",
    "def log_results(epoch, train_loss, test_loss, acc, f1, precision, recall):\n",
    "    file_exists = os.path.isfile(CSV_LOG)\n",
    "    with open(CSV_LOG, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'train_loss', 'test_loss', 'test_acc', 'test_f1', 'test_precision', 'test_recall'])\n",
    "        writer.writerow([epoch, train_loss, test_loss, acc, f1, precision, recall])\n",
    "\n",
    "# -----------------------------\n",
    "# Run AdamW Optimizer Experiment (F)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_baseline_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n",
    "        test_loss, test_acc, test_f1, test_precision, test_recall = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "\n",
    "        log_results(epoch+1, train_loss, test_loss, test_acc, test_f1, test_precision, test_recall)\n",
    "\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(\"✅ Best model saved!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNjIgQe4Tlg9BcgolGpFV9/",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
