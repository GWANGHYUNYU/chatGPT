{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Eivf9Ur06vn"
   },
   "source": [
    "# PyTorch Lightning 스타일로 코딩\n",
    "\n",
    "1. **모델 정의 (MLP 클래스)**\n",
    "\n",
    "- `pl.LightningModule`를 상속하여 모델 클래스 정의.\n",
    "\n",
    "- `training_step()`, `validation_step()`, `configure_optimizers()` 메서드 구현.\n",
    "\n",
    "2. **콜백 (Callback)**\n",
    "\n",
    "- `ModelCheckpoint` 콜백을 사용하여 `val_loss`가 가장 낮은 모델을 저장.\n",
    "\n",
    "3. **학습 및 테스트** (`Trainer`):\n",
    "```python\n",
    "trainer = Trainer(max_epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "trainer.test(model, test_loader)\n",
    "```\n",
    "\n",
    "4. **주요 기능**\n",
    "\n",
    "- `ModelCheckpoint` 콜백을 통해 모델이 `best_model.ckpt` 파일로 자동 저장됩니다.\n",
    "\n",
    " - 학습 루프가 깔끔하게 정리되었고, 학습률 스케줄러도 `configure_optimizers()`내에서 정의됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUUOB6uE08or"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torchviz import make_dot\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # 에폭 수 증가\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5  # 가중치 감쇠를 줄임\n",
    "DROPOUT_RATE = 0.3  # 드롭아웃 확률을 낮춤\n",
    "INPUT_SIZE = 3 * 32 * 32  # CIFAR10 이미지의 Flatten 크기\n",
    "NUM_CLASSES = 10  # CIFAR10은 10개의 클래스\n",
    "MODEL_PATH = './best_model.ckpt'  # 모델 저장 경로\n",
    "\n",
    "# CIFAR-10 데이터셋의 실제 평균과 표준편차 사용\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# 데이터 전처리 정의\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\", dirpath=\"./\", filename=\"best_model\")\n",
    "\n",
    "model = MLP()\n",
    "trainer = Trainer(max_epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "\n",
    "# 학습 실행\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# 테스트 실행\n",
    "trainer.test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk3ptl0i9m_L"
   },
   "source": [
    "## ✅ Pytorch Lightning에 Optuna 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwBOTLEG9nV3"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import optuna\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.ckpt'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "\n",
    "class MLP(LightningModule):\n",
    "    def __init__(self, learning_rate=LEARNING_RATE, dropout_rate=DROPOUT_RATE):\n",
    "        super(MLP, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.precision = Precision(average='macro', num_classes=NUM_CLASSES)\n",
    "        self.recall = Recall(average='macro', num_classes=NUM_CLASSES)\n",
    "        self.f1 = F1Score(average='macro', num_classes=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.accuracy(preds, y), prog_bar=True)\n",
    "        self.log('val_precision', self.precision(preds, y), prog_bar=True)\n",
    "        self.log('val_recall', self.recall(preds, y), prog_bar=True)\n",
    "        self.log('val_f1', self.f1(preds, y), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.accuracy(preds, y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "    model = MLP(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_acc\", patience=PATIENCE, mode=\"max\"),\n",
    "            PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")\n",
    "        ],\n",
    "        enable_checkpointing=False,\n",
    "        logger=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics['val_acc'].item()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_size = int(SPLIT_RATIO * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(\"Best Hyperparameters:\", study.best_params)\n",
    "    print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "    # 테스트 단계\n",
    "    model = MLP.load_from_checkpoint(MODEL_PATH)\n",
    "    trainer = Trainer()\n",
    "    trainer.test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZkzOxRF9u8K"
   },
   "source": [
    "## ✅ GPU 설정 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iugQnjLT9uUK"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import optuna\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.ckpt'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 데이터셋의 평균 및 표준편차 (정규화 기준)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "\n",
    "class MLP(LightningModule):\n",
    "    def __init__(self, learning_rate=LEARNING_RATE, dropout_rate=DROPOUT_RATE):\n",
    "        super(MLP, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.precision = Precision(average='macro', num_classes=NUM_CLASSES)\n",
    "        self.recall = Recall(average='macro', num_classes=NUM_CLASSES)\n",
    "        self.f1 = F1Score(average='macro', num_classes=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.accuracy(preds, y), prog_bar=True)\n",
    "        self.log('val_precision', self.precision(preds, y), prog_bar=True)\n",
    "        self.log('val_recall', self.recall(preds, y), prog_bar=True)\n",
    "        self.log('val_f1', self.f1(preds, y), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.accuracy(preds, y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "    model = MLP(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "    trainer = Trainer(accelerator=\"gpu\", devices=1,\n",
    "        max_epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_acc\", patience=PATIENCE, mode=\"max\"),\n",
    "            PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")\n",
    "        ],\n",
    "        enable_checkpointing=False,\n",
    "        logger=False,\n",
    "        accelerator=\"gpu\",  # GPU 사용 설정\n",
    "        devices=1           # 사용할 GPU 개수\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics['val_acc'].item()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_size = int(SPLIT_RATIO * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(\"Best Hyperparameters:\", study.best_params)\n",
    "    print(\"Best Validation Accuracy:\", study.best_value)\n",
    "\n",
    "    # 테스트 단계\n",
    "    model = MLP.load_from_checkpoint(MODEL_PATH)\n",
    "    trainer = Trainer(accelerator=\"gpu\", devices=1,accelerator=\"gpu\", devices=1)\n",
    "    trainer.test(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO9i5N4cgvbnyZ08CKsty0d",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
