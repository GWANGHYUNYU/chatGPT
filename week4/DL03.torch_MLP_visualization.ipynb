{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNENI_9S_8oZ"
   },
   "source": [
    "## üß† torchsummary\n",
    "\n",
    "### ‚úÖ ÏÑ§Ïπò Î∞©Î≤ï\n",
    "`pip install torchsummary`\n",
    "\n",
    "### ‚úÖ Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï\n",
    "- `input_size`: (channels, height, width)\n",
    "\n",
    "```python\n",
    "from torchsummary import summary\n",
    "\n",
    "# Ïòà: Î™®Îç∏ Ï†ïÏùò ÌõÑ\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# summary Ìò∏Ï∂ú (Ïòà: CIFAR10 Ïù¥ÎØ∏ÏßÄ 3x32x32)\n",
    "summary(model, input_size=(3, 32, 32))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281389,
     "status": "ok",
     "timestamp": 1743689487041,
     "user": {
      "displayName": "Ïú†Í¥ëÌòÑ",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "9Z9_HDZt4gSR",
    "outputId": "96cf0ac9-f5b6-48bd-fb0c-3fbef0c2f5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-0d44eb106e74>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:13<00:00, 12.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                 [-1, 3072]               0\n",
      "            Linear-2                  [-1, 512]       1,573,376\n",
      "       BatchNorm1d-3                  [-1, 512]           1,024\n",
      "              ReLU-4                  [-1, 512]               0\n",
      "           Dropout-5                  [-1, 512]               0\n",
      "            Linear-6                  [-1, 256]         131,328\n",
      "       BatchNorm1d-7                  [-1, 256]             512\n",
      "              ReLU-8                  [-1, 256]               0\n",
      "           Dropout-9                  [-1, 256]               0\n",
      "           Linear-10                  [-1, 256]          65,792\n",
      "      BatchNorm1d-11                  [-1, 256]             512\n",
      "             ReLU-12                  [-1, 256]               0\n",
      "          Dropout-13                  [-1, 256]               0\n",
      "           Linear-14                  [-1, 128]          32,896\n",
      "      BatchNorm1d-15                  [-1, 128]             256\n",
      "             ReLU-16                  [-1, 128]               0\n",
      "           Linear-17                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,806,986\n",
      "Trainable params: 1,806,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 6.89\n",
      "Estimated Total Size (MB): 6.96\n",
      "----------------------------------------------------------------\n",
      "Validation - Precision: 0.4376, Recall: 0.4366, F1 Score: 0.4251\n",
      "Epoch [1/20], Train Loss: 1.7696, Val Loss: 1.5661, Val Acc: 43.65%\n",
      "New Best Model Saved! Validation Accuracy: 43.65%\n",
      "Validation - Precision: 0.4754, Recall: 0.4750, F1 Score: 0.4683\n",
      "Epoch [2/20], Train Loss: 1.5781, Val Loss: 1.4657, Val Acc: 47.45%\n",
      "New Best Model Saved! Validation Accuracy: 47.45%\n",
      "Validation - Precision: 0.5015, Recall: 0.5029, F1 Score: 0.4992\n",
      "Epoch [3/20], Train Loss: 1.4916, Val Loss: 1.4014, Val Acc: 50.24%\n",
      "New Best Model Saved! Validation Accuracy: 50.24%\n",
      "Validation - Precision: 0.5120, Recall: 0.5145, F1 Score: 0.5086\n",
      "Epoch [4/20], Train Loss: 1.4316, Val Loss: 1.3604, Val Acc: 51.37%\n",
      "New Best Model Saved! Validation Accuracy: 51.37%\n",
      "Validation - Precision: 0.5172, Recall: 0.5204, F1 Score: 0.5159\n",
      "Epoch [5/20], Train Loss: 1.3915, Val Loss: 1.3470, Val Acc: 51.97%\n",
      "New Best Model Saved! Validation Accuracy: 51.97%\n",
      "Validation - Precision: 0.5402, Recall: 0.5424, F1 Score: 0.5402\n",
      "Epoch [6/20], Train Loss: 1.3093, Val Loss: 1.2831, Val Acc: 54.19%\n",
      "New Best Model Saved! Validation Accuracy: 54.19%\n",
      "Validation - Precision: 0.5475, Recall: 0.5477, F1 Score: 0.5457\n",
      "Epoch [7/20], Train Loss: 1.2712, Val Loss: 1.2667, Val Acc: 54.70%\n",
      "New Best Model Saved! Validation Accuracy: 54.70%\n",
      "Validation - Precision: 0.5469, Recall: 0.5498, F1 Score: 0.5473\n",
      "Epoch [8/20], Train Loss: 1.2456, Val Loss: 1.2681, Val Acc: 54.91%\n",
      "New Best Model Saved! Validation Accuracy: 54.91%\n",
      "Validation - Precision: 0.5571, Recall: 0.5591, F1 Score: 0.5576\n",
      "Epoch [9/20], Train Loss: 1.2207, Val Loss: 1.2480, Val Acc: 55.85%\n",
      "New Best Model Saved! Validation Accuracy: 55.85%\n",
      "Validation - Precision: 0.5543, Recall: 0.5556, F1 Score: 0.5522\n",
      "Epoch [10/20], Train Loss: 1.1925, Val Loss: 1.2538, Val Acc: 55.50%\n",
      "Validation - Precision: 0.5660, Recall: 0.5674, F1 Score: 0.5662\n",
      "Epoch [11/20], Train Loss: 1.1361, Val Loss: 1.2236, Val Acc: 56.69%\n",
      "New Best Model Saved! Validation Accuracy: 56.69%\n",
      "Validation - Precision: 0.5666, Recall: 0.5683, F1 Score: 0.5672\n",
      "Epoch [12/20], Train Loss: 1.1135, Val Loss: 1.2183, Val Acc: 56.80%\n",
      "New Best Model Saved! Validation Accuracy: 56.80%\n",
      "Validation - Precision: 0.5659, Recall: 0.5695, F1 Score: 0.5669\n",
      "Epoch [13/20], Train Loss: 1.0939, Val Loss: 1.2173, Val Acc: 56.88%\n",
      "New Best Model Saved! Validation Accuracy: 56.88%\n",
      "Validation - Precision: 0.5735, Recall: 0.5712, F1 Score: 0.5716\n",
      "Epoch [14/20], Train Loss: 1.0796, Val Loss: 1.2228, Val Acc: 57.07%\n",
      "New Best Model Saved! Validation Accuracy: 57.07%\n",
      "Validation - Precision: 0.5707, Recall: 0.5697, F1 Score: 0.5692\n",
      "Epoch [15/20], Train Loss: 1.0637, Val Loss: 1.2194, Val Acc: 56.92%\n",
      "Validation - Precision: 0.5767, Recall: 0.5782, F1 Score: 0.5771\n",
      "Epoch [16/20], Train Loss: 1.0251, Val Loss: 1.2057, Val Acc: 57.78%\n",
      "New Best Model Saved! Validation Accuracy: 57.78%\n",
      "Validation - Precision: 0.5738, Recall: 0.5767, F1 Score: 0.5747\n",
      "Epoch [17/20], Train Loss: 1.0057, Val Loss: 1.2085, Val Acc: 57.60%\n",
      "Validation - Precision: 0.5785, Recall: 0.5808, F1 Score: 0.5792\n",
      "Epoch [18/20], Train Loss: 1.0012, Val Loss: 1.2134, Val Acc: 58.02%\n",
      "New Best Model Saved! Validation Accuracy: 58.02%\n",
      "Validation - Precision: 0.5800, Recall: 0.5793, F1 Score: 0.5790\n",
      "Epoch [19/20], Train Loss: 0.9878, Val Loss: 1.2198, Val Acc: 57.90%\n",
      "Validation - Precision: 0.5808, Recall: 0.5818, F1 Score: 0.5810\n",
      "Epoch [20/20], Train Loss: 0.9747, Val Loss: 1.2125, Val Acc: 58.12%\n",
      "New Best Model Saved! Validation Accuracy: 58.12%\n",
      "Test Accuracy: 57.53%\n",
      "Test Precision: 0.5743, Recall: 0.5753, F1 Score: 0.5744\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchsummary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# GPU ÏÑ§Ï†ï\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌèâÍ∑† Î∞è ÌëúÏ§ÄÌé∏Ï∞® (Ï†ïÍ∑úÌôî Í∏∞Ï§Ä)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏÑ§Ï†ï (ÌõÑÎ∞ò ÌïôÏäµÏö©)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò (Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏóÜÏù¥ Ï†ïÍ∑úÌôîÎßå Ï†ÅÏö©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÎ¶¨\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader Ï†ïÏùò\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Î™®Îç∏ Ï†ïÏùò (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# summary Ìò∏Ï∂ú (Ïòà: CIFAR10 Ïù¥ÎØ∏ÏßÄ 3x32x32)\n",
    "summary(model, input_size=(3, 32, 32))\n",
    "\n",
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# ÌïôÏäµ Ìï®Ïàò Ï†ïÏùò\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping Ï≤¥ÌÅ¨\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # Î∞òÌôò:(ÏµúÎåìÍ∞í, Ïù∏Îç±Ïä§)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# ÏµúÏ†ÅÏùò ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï†ïÎ≥¥ÏôÄ ÌôòÍ≤Ω Ï†ïÎ≥¥ Ï†ÄÏû• Ìï®Ïàò\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    dataset_name = 'CIFAR10'  # ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Î¶Ñ\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # Î™®Îç∏ Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # Ï†ïÎ≥¥ Ï†ÄÏû•\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # ÏµúÏ†ÅÏùò Ï†ïÎ≥¥ Ï†ÄÏû•\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kog0J0T1-aEE"
   },
   "source": [
    "## üß† torchinfo\n",
    "\n",
    "### ‚úÖ ÏÑ§Ïπò Î∞©Î≤ï\n",
    "`pip install torchinfo`\n",
    "\n",
    "### ‚úÖ Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï\n",
    "- `input_size`: Î∞∞Ïπò Ï∞®Ïõê Ìè¨Ìï® (batch_size, channels, height, width)\n",
    "\n",
    "```python\n",
    "from torchinfo import summary\n",
    "\n",
    "# Î™®Îç∏ ÏÉùÏÑ±\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# summary Ìò∏Ï∂ú\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129621,
     "status": "ok",
     "timestamp": 1743690199343,
     "user": {
      "displayName": "Ïú†Í¥ëÌòÑ",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "YIF2nNF--dyq",
    "outputId": "a9cd5121-d0cd-4c38-ed4f-a3dd36bc1b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLP                                      [128, 10]                 --\n",
      "‚îú‚îÄSequential: 1-1                        [128, 10]                 --\n",
      "‚îÇ    ‚îî‚îÄFlatten: 2-1                      [128, 3072]               --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-2                       [128, 512]                1,573,376\n",
      "‚îÇ    ‚îî‚îÄBatchNorm1d: 2-3                  [128, 512]                1,024\n",
      "‚îÇ    ‚îî‚îÄReLU: 2-4                         [128, 512]                --\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-5                      [128, 512]                --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-6                       [128, 256]                131,328\n",
      "‚îÇ    ‚îî‚îÄBatchNorm1d: 2-7                  [128, 256]                512\n",
      "‚îÇ    ‚îî‚îÄReLU: 2-8                         [128, 256]                --\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-9                      [128, 256]                --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-10                      [128, 256]                65,792\n",
      "‚îÇ    ‚îî‚îÄBatchNorm1d: 2-11                 [128, 256]                512\n",
      "‚îÇ    ‚îî‚îÄReLU: 2-12                        [128, 256]                --\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-13                     [128, 256]                --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-14                      [128, 128]                32,896\n",
      "‚îÇ    ‚îî‚îÄBatchNorm1d: 2-15                 [128, 128]                256\n",
      "‚îÇ    ‚îî‚îÄReLU: 2-16                        [128, 128]                --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-17                      [128, 10]                 1,290\n",
      "==========================================================================================\n",
      "Total params: 1,806,986\n",
      "Trainable params: 1,806,986\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 231.29\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 2.37\n",
      "Params size (MB): 7.23\n",
      "Estimated Total Size (MB): 11.17\n",
      "==========================================================================================\n",
      "Validation - Precision: 0.4397, Recall: 0.4423, F1 Score: 0.4349\n",
      "Epoch [1/10], Train Loss: 1.7745, Val Loss: 1.5512, Val Acc: 44.17%\n",
      "New Best Model Saved! Validation Accuracy: 44.17%\n",
      "Validation - Precision: 0.4750, Recall: 0.4785, F1 Score: 0.4690\n",
      "Epoch [2/10], Train Loss: 1.5796, Val Loss: 1.4625, Val Acc: 47.85%\n",
      "New Best Model Saved! Validation Accuracy: 47.85%\n",
      "Validation - Precision: 0.5004, Recall: 0.5025, F1 Score: 0.4980\n",
      "Epoch [3/10], Train Loss: 1.4967, Val Loss: 1.4087, Val Acc: 50.22%\n",
      "New Best Model Saved! Validation Accuracy: 50.22%\n",
      "Validation - Precision: 0.5062, Recall: 0.5091, F1 Score: 0.5022\n",
      "Epoch [4/10], Train Loss: 1.4397, Val Loss: 1.3737, Val Acc: 50.85%\n",
      "New Best Model Saved! Validation Accuracy: 50.85%\n",
      "Validation - Precision: 0.5177, Recall: 0.5220, F1 Score: 0.5179\n",
      "Epoch [5/10], Train Loss: 1.3949, Val Loss: 1.3377, Val Acc: 52.13%\n",
      "New Best Model Saved! Validation Accuracy: 52.13%\n",
      "Validation - Precision: 0.5375, Recall: 0.5400, F1 Score: 0.5373\n",
      "Epoch [6/10], Train Loss: 1.3094, Val Loss: 1.2947, Val Acc: 53.93%\n",
      "New Best Model Saved! Validation Accuracy: 53.93%\n",
      "Validation - Precision: 0.5430, Recall: 0.5428, F1 Score: 0.5380\n",
      "Epoch [7/10], Train Loss: 1.2745, Val Loss: 1.2838, Val Acc: 54.21%\n",
      "New Best Model Saved! Validation Accuracy: 54.21%\n",
      "Validation - Precision: 0.5417, Recall: 0.5451, F1 Score: 0.5402\n",
      "Epoch [8/10], Train Loss: 1.2467, Val Loss: 1.2790, Val Acc: 54.41%\n",
      "New Best Model Saved! Validation Accuracy: 54.41%\n",
      "Validation - Precision: 0.5453, Recall: 0.5469, F1 Score: 0.5430\n",
      "Epoch [9/10], Train Loss: 1.2226, Val Loss: 1.2680, Val Acc: 54.65%\n",
      "New Best Model Saved! Validation Accuracy: 54.65%\n",
      "Validation - Precision: 0.5555, Recall: 0.5548, F1 Score: 0.5517\n",
      "Epoch [10/10], Train Loss: 1.1983, Val Loss: 1.2558, Val Acc: 55.36%\n",
      "New Best Model Saved! Validation Accuracy: 55.36%\n",
      "Test Accuracy: 54.82%\n",
      "Test Precision: 0.5508, Recall: 0.5482, F1 Score: 0.5465\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchinfo import summary\n",
    "\n",
    "# GPU ÏÑ§Ï†ï\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌèâÍ∑† Î∞è ÌëúÏ§ÄÌé∏Ï∞® (Ï†ïÍ∑úÌôî Í∏∞Ï§Ä)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏÑ§Ï†ï (ÌõÑÎ∞ò ÌïôÏäµÏö©)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò (Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏóÜÏù¥ Ï†ïÍ∑úÌôîÎßå Ï†ÅÏö©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÎ¶¨\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader Ï†ïÏùò\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Î™®Îç∏ Ï†ïÏùò (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# summary Ìò∏Ï∂ú\n",
    "print(summary(model, input_size=(BATCH_SIZE, 3, 32, 32)))\n",
    "\n",
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# ÌïôÏäµ Ìï®Ïàò Ï†ïÏùò\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping Ï≤¥ÌÅ¨\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # Î∞òÌôò:(ÏµúÎåìÍ∞í, Ïù∏Îç±Ïä§)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# ÏµúÏ†ÅÏùò ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï†ïÎ≥¥ÏôÄ ÌôòÍ≤Ω Ï†ïÎ≥¥ Ï†ÄÏû• Ìï®Ïàò\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    dataset_name = 'CIFAR10'  # ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Î¶Ñ\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # Î™®Îç∏ Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # Ï†ïÎ≥¥ Ï†ÄÏû•\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # ÏµúÏ†ÅÏùò Ï†ïÎ≥¥ Ï†ÄÏû•\n",
    "    save_study_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee6gehVAA2S1"
   },
   "source": [
    "## üß† torchviz\n",
    "\n",
    "### ‚úÖ ÏÑ§Ïπò Î∞©Î≤ï\n",
    "`pip install torchviz`\n",
    "\n",
    "### ‚úÖ ÏÇ¨Ïö©Î≤ï ÏöîÏïΩ\n",
    "- `make_dot()`ÏùÑ ÏÇ¨Ïö©Ìï¥ÏÑú Î™®Îç∏Ïùò Í∑∏ÎûòÌîÑÎ•º ÏÉùÏÑ±\n",
    "\n",
    "```python\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Î™®Îç∏ Î∞è ÎçîÎØ∏ ÏûÖÎ†•\n",
    "model = MLP().to(DEVICE)\n",
    "dummy_input = torch.randn(BATCH_SIZE, 3, 32, 32).to(DEVICE)  # Î∞∞Ïπò ÌÅ¨Í∏∞ Î™®Îç∏Í≥º ÎèôÏùº\n",
    "\n",
    "# forward pass (output)\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# Ï†ÄÏû• (PDF or PNG Îì±ÏúºÎ°ú)\n",
    "dot.render(\"model_graph\", format=\"png\")  # model_graph.png ÏÉùÏÑ±Îê®\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127886,
     "status": "ok",
     "timestamp": 1743690808209,
     "user": {
      "displayName": "Ïú†Í¥ëÌòÑ",
      "userId": "16629314767644566100"
     },
     "user_tz": -540
    },
    "id": "1jNgIf8v_PDP",
    "outputId": "9c4b89eb-b737-4133-d936-f5139c989804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
      "Validation - Precision: 0.4407, Recall: 0.4458, F1 Score: 0.4352\n",
      "Epoch [1/10], Train Loss: 1.7784, Val Loss: 1.5436, Val Acc: 44.55%\n",
      "New Best Model Saved! Validation Accuracy: 44.55%\n",
      "Validation - Precision: 0.4683, Recall: 0.4678, F1 Score: 0.4579\n",
      "Epoch [2/10], Train Loss: 1.5808, Val Loss: 1.4770, Val Acc: 46.83%\n",
      "New Best Model Saved! Validation Accuracy: 46.83%\n",
      "Validation - Precision: 0.4917, Recall: 0.4952, F1 Score: 0.4902\n",
      "Epoch [3/10], Train Loss: 1.4953, Val Loss: 1.4091, Val Acc: 49.52%\n",
      "New Best Model Saved! Validation Accuracy: 49.52%\n",
      "Validation - Precision: 0.5044, Recall: 0.5034, F1 Score: 0.5010\n",
      "Epoch [4/10], Train Loss: 1.4380, Val Loss: 1.3806, Val Acc: 50.31%\n",
      "New Best Model Saved! Validation Accuracy: 50.31%\n",
      "Validation - Precision: 0.5165, Recall: 0.5162, F1 Score: 0.5128\n",
      "Epoch [5/10], Train Loss: 1.3930, Val Loss: 1.3466, Val Acc: 51.66%\n",
      "New Best Model Saved! Validation Accuracy: 51.66%\n",
      "Validation - Precision: 0.5325, Recall: 0.5324, F1 Score: 0.5308\n",
      "Epoch [6/10], Train Loss: 1.3062, Val Loss: 1.3025, Val Acc: 53.29%\n",
      "New Best Model Saved! Validation Accuracy: 53.29%\n",
      "Validation - Precision: 0.5384, Recall: 0.5425, F1 Score: 0.5395\n",
      "Epoch [7/10], Train Loss: 1.2719, Val Loss: 1.2789, Val Acc: 54.26%\n",
      "New Best Model Saved! Validation Accuracy: 54.26%\n",
      "Validation - Precision: 0.5383, Recall: 0.5385, F1 Score: 0.5356\n",
      "Epoch [8/10], Train Loss: 1.2385, Val Loss: 1.2838, Val Acc: 53.87%\n",
      "Validation - Precision: 0.5509, Recall: 0.5529, F1 Score: 0.5506\n",
      "Epoch [9/10], Train Loss: 1.2178, Val Loss: 1.2689, Val Acc: 55.33%\n",
      "New Best Model Saved! Validation Accuracy: 55.33%\n",
      "Validation - Precision: 0.5478, Recall: 0.5518, F1 Score: 0.5483\n",
      "Epoch [10/10], Train Loss: 1.1905, Val Loss: 1.2600, Val Acc: 55.22%\n",
      "Test Accuracy: 54.76%\n",
      "Test Precision: 0.5448, Recall: 0.5476, F1 Score: 0.5448\n",
      "Best study info saved to best_study_info.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchviz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import platform\n",
    "import pkg_resources\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchviz import make_dot\n",
    "\n",
    "# GPU ÏÑ§Ï†ï\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.3\n",
    "INPUT_SIZE = 3 * 32 * 32\n",
    "NUM_CLASSES = 10\n",
    "MODEL_PATH = './best_model.pth'\n",
    "PATIENCE = 3\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "# CIFAR10 Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌèâÍ∑† Î∞è ÌëúÏ§ÄÌé∏Ï∞® (Ï†ïÍ∑úÌôî Í∏∞Ï§Ä)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏÑ§Ï†ï (ÌõÑÎ∞ò ÌïôÏäµÏö©)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò (Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏóÜÏù¥ Ï†ïÍ∑úÌôîÎßå Ï†ÅÏö©)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "])\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "full_train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test, download=True)\n",
    "\n",
    "# ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÎ¶¨\n",
    "train_size = int(SPLIT_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test\n",
    "\n",
    "# DataLoader Ï†ïÏùò\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Î™®Îç∏ Ï†ïÏùò (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_SIZE, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "# torchvizÎ°ú Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "\n",
    "dummy_input = torch.randn(BATCH_SIZE, 3, 32, 32).to(DEVICE)  # Î∞∞Ïπò ÌÅ¨Í∏∞ 1\n",
    "\n",
    "output = model(dummy_input) # forward pass (output)\n",
    "\n",
    "dot = make_dot(output, params=dict(model.named_parameters())) # Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\n",
    "\n",
    "dot.render(\"model_graph\", format=\"png\")  # model_graph.png ÏÉùÏÑ±Îê®\n",
    "\n",
    "\n",
    "\n",
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        print('Model loaded from checkpoint.')\n",
    "    else:\n",
    "        print('No checkpoint found, training from scratch.')\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# ÌïôÏäµ Ìï®Ïàò Ï†ïÏùò\n",
    "def train():\n",
    "    global best_val_acc, patience_counter\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_loss, val_acc = validate()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:  # Early Stopping Ï≤¥ÌÅ¨\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"New Best Model Saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) # Î∞òÌôò:(ÏµúÎåìÍ∞í, Ïù∏Îç±Ïä§)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# ÏµúÏ†ÅÏùò ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï†ïÎ≥¥ÏôÄ ÌôòÍ≤Ω Ï†ïÎ≥¥ Ï†ÄÏû• Ìï®Ïàò\n",
    "def save_study_info(filename=\"best_study_info.json\"):\n",
    "\n",
    "    # Python Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "    python_version = platform.python_version()\n",
    "    platform_info = platform.platform()\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    dataset_name = 'CIFAR10'  # ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Î¶Ñ\n",
    "    dataset_info = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"validation_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"image_size\": (32, 32),  # CIFAR-10 Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # Î™®Îç∏ Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    model_info = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_structure\": str(model),\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    }\n",
    "\n",
    "    # Ï†ïÎ≥¥ Ï†ÄÏû•\n",
    "    data = {\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "        \"installed_packages\": installed_packages,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"model_info\": model_info\n",
    "    }\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"Best study info saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_model()\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # ÏµúÏ†ÅÏùò Ï†ïÎ≥¥ Ï†ÄÏû•\n",
    "    save_study_info()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMmBLuDAZI2YaFJL6pxcNjY",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
